{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Spark SQL and its performance using TPC-DS workload\n",
    "This notebook sets up the Spark environment to run TPC-DS bench-mark on 1GB scale factor. TPC-DS is a widely used industry standard decision support benchmark that is used to evaluate performance of the data processing engines. Given TPC-DS excercises some key data warehouse features, running TPC-DS successfully reflects the readiness of Spark in terms of addressing the need of a data warehouse application. Apache Spark v2.0 supports all 99 decision support queries that is part of this benchmark. \n",
    "\n",
    "This notebook is written in scala and is intended to help the spark developers gain understanding on the setup steps required to run the benchmark.\n",
    "<b>Please note :</b> Several additional tuning steps may be required when adapting this to an actual spark production cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the journey data\n",
    "- Clone the tpcds journey repository to get access to all the data and scripts that are required to excercise this journey. \n",
    "- Normally the data and queries are generated by running the data and query generation utility from the tpcds toolkit available at http://www.tpc.org/tpcds. However for ease of use, the data and queries are pre-generated for 1GB scale factor. \n",
    "- We use the pre-generated data and queries to demonstrate how they can be used to run the tpcds queries against spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached version of spark-kernel-brunel-all-2.4.jar\n",
      "git version 1.8.3.1\n",
      "Cloning into 'spark-tpc-ds-performance-test'...\n",
      "remote: Counting objects: 886, done.        \n",
      "remote: Compressing objects: 100% (40/40), done.        \n",
      "remote: Total 886 (delta 56), reused 58 (delta 42), pack-reused 802        ts:  48% (426/886), 351.89 MiB | 17.94 MiB/s   ng objects:  53% (470/886), 351.89 MiB | 17.94 MiB/s   \n",
      "Receiving objects: 100% (886/886), 363.15 MiB | 18.41 MiB/s, done.\n",
      "Resolving deltas: 100% (338/338), done.\n",
      "Checking out files: 100% (810/810), done.\n"
     ]
    }
   ],
   "source": [
    "import sys.process._\n",
    "%AddJar -magic https://brunelvis.org/jar/spark-kernel-brunel-all-2.4.jar\n",
    "\"rm -rf spark-tpc-ds-performance-test\" !\n",
    "\"git --version\" !\n",
    "\"git clone --progress https://github.com/IBM/spark-tpc-ds-performance-test.git\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup variables.\n",
    "* Sets up variables that are used in the rest of this notebook.\n",
    "* The path variables are relative to the git clone directory.\n",
    "* tpcdsDatabaseName is hard-coded to \"TPCDS1G\". This can be changed if a different database name is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPCDS root directory is at : spark-tpc-ds-performance-test\n",
      "TPCDS ddl scripts directory is at: spark-tpc-ds-performance-test/src/ddl/individual\n",
      "TPCDS data directory is at: spark-tpc-ds-performance-test/src/data\n",
      "TPCDS queries directory is at: spark-tpc-ds-performance-test/src/queries\n"
     ]
    }
   ],
   "source": [
    "val tpcdsRootDir = \"spark-tpc-ds-performance-test\"\n",
    "val tpcdsWorkDir = \"spark-tpc-ds-performance-test/work\"\n",
    "val tpcdsDdlDir = s\"${tpcdsRootDir}/src/ddl/individual\"\n",
    "val tpcdsGenDataDir = s\"${tpcdsRootDir}/src/data\"\n",
    "val tpcdsQueriesDir = s\"${tpcdsRootDir}/src/queries\"\n",
    "val tpcdsDatabaseName = \"TPCDS1G\"\n",
    "var totalTime: Long = 0\n",
    "println(\"TPCDS root directory is at : \"+ tpcdsRootDir)\n",
    "println(\"TPCDS ddl scripts directory is at: \" + tpcdsDdlDir)\n",
    "println(\"TPCDS data directory is at: \"+ tpcdsGenDataDir)\n",
    "println(\"TPCDS queries directory is at: \"+ tpcdsQueriesDir)\n",
    "val spark = SparkSession.\n",
    "    builder().\n",
    "    config(\"spark.ui.showConsoleProgress\", false).\n",
    "    config(\"spark.sql.autoBroadcastJoinThreshold\", -1).\n",
    "    config(\"spark.sql.crossJoin.enabled\", true).\n",
    "    getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function definitions.\n",
    "* Defines the utility functions that are called from the cells below in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clearTableDirectory(tableName: String): Unit = {\n",
    "    import sys.process._\n",
    "    val commandStr1 = s\"rm -rf spark-warehouse/tpcds2g.db/${tableName}/*\"\n",
    "    val commandStr2 = s\"rm -rf spark-warehouse/tpcds2g.db/${tableName}\"\n",
    "    var exitCode = Process(commandStr1).!\n",
    "    exitCode = Process(commandStr2).!\n",
    "}\n",
    "\n",
    "def createDatabase(): Unit = {\n",
    "    spark.sql(s\"DROP DATABASE IF EXISTS ${tpcdsDatabaseName} CASCADE\")\n",
    "    spark.sql(s\"CREATE DATABASE ${tpcdsDatabaseName}\")\n",
    "    spark.sql(s\"USE ${tpcdsDatabaseName}\")\n",
    "}\n",
    "\n",
    "/**\n",
    " * Function to create a table in spark. It reads the DDL script for each of the\n",
    " * tpc-ds table and executes it on Spark.\n",
    " */\n",
    "def createTable(tableName: String): Unit = {\n",
    "  println(s\"Creating table $tableName ..\")\n",
    "  spark.sql(s\"DROP TABLE IF EXISTS $tableName\")\n",
    "  clearTableDirectory(tableName)  \n",
    "  val (fileName, content) = \n",
    "    spark.sparkContext.wholeTextFiles(s\"${tpcdsDdlDir}/$tableName.sql\").collect()(0) \n",
    "    \n",
    "  // Remove the replace for the .dat once it is fixed in the github repo\n",
    "  val sqlStmts = content.stripLineEnd\n",
    "    .replace('\\n', ' ')\n",
    "    .replace(\"${TPCDS_GENDATA_DIR}\", tpcdsGenDataDir)\n",
    "    .replace(\"csv\", \"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").split(\";\")\n",
    "  sqlStmts.map(stmt => spark.sql(stmt))    \n",
    "}  \n",
    "\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.sql.DataFrame\n",
    "\n",
    "def runQuery(queryStr: String,\n",
    "             individual: Boolean = true,\n",
    "             resultDir: String): Seq[(String, Double, Int, String)] = {\n",
    "  val querySummary = ArrayBuffer.empty[(String, Double, Int, String)]  \n",
    "  val queryName = s\"${tpcdsQueriesDir}/query${queryStr}.sql\"   \n",
    "  val (_, content) = spark.sparkContext.wholeTextFiles(queryName).collect()(0)  \n",
    "  val queries = content.split(\"\\n\")\n",
    "    .filterNot (_.startsWith(\"--\"))\n",
    "    .mkString(\" \").split(\";\")\n",
    "  \n",
    "  var cnt = 1  \n",
    "  for (query <- queries)  {\n",
    "   val start = System.nanoTime()\n",
    "   val df = spark.sql(query)   \n",
    "   val result = spark.sql(query).collect  \n",
    "   val timeElapsed = (System.nanoTime() - start) / 1000000000\n",
    "   val name = if (queries.length > 1) {\n",
    "       s\"query${queryStr}-${cnt}\"\n",
    "   } else {\n",
    "       s\"query${queryStr}\"\n",
    "   }  \n",
    "   val resultFile = s\"${resultDir}/${name}-notebook.res\"  \n",
    "   df.coalesce(1)\n",
    "      .write.format(\"com.databricks.spark.csv\")\n",
    "      .option(\"header\", \"true\")\n",
    "      .mode(\"overwrite\")\n",
    "      .save(resultFile)\n",
    "   totalTime = totalTime + timeElapsed\n",
    "  \n",
    "   querySummary += Tuple4.apply(name, timeElapsed, result.length, resultFile)\n",
    "   cnt += 1                \n",
    "  }\n",
    "  querySummary \n",
    "}\n",
    "\n",
    "// run function for each table in tables array\n",
    "def forEachTable(tables: Array[String], f: (String) => Unit): Unit = {\n",
    "  for ( table <- tables) {\n",
    "    try {\n",
    "      f(table)\n",
    "    } catch {\n",
    "      case e: Throwable => {\n",
    "        println(\"EXCEPTION!! \" + e.getMessage())\n",
    "        throw e\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "def runIndividualQuery(queryNum: Int, resultDir: String = tpcdsWorkDir ): DataFrame = {\n",
    "    val queryStr = \"%02d\".format(queryNum) \n",
    "    val testSummary = ArrayBuffer.empty[(String, Double, Int, String)] \n",
    "    try {      \n",
    "      println(s\"Running TPC-DS Query : $queryStr\")  \n",
    "      testSummary ++= runQuery(queryStr, true, resultDir)\n",
    "    } catch {\n",
    "        case e: Throwable => {\n",
    "            println(\"Error in query \"+ queryNum + \" msg = \" + e.getMessage)\n",
    "        }\n",
    "    }\n",
    "    testSummary.toDF(\"QueryName\",\"ElapsedTime\",\"RowsReturned\", \"ResultLocation\")\n",
    "}\n",
    "\n",
    "def runAllQueries(resultDir: String = tpcdsWorkDir): DataFrame = {\n",
    "  val testSummary = ArrayBuffer.empty[(String, Double, Int, String)]    \n",
    "  var queryErrors = 0\n",
    "  for (i <- 1 to 99) {\n",
    "    try{\n",
    "      val queryStr = \"%02d\".format(i)\n",
    "      println(s\"Running TPC-DS Query : $queryStr\")   \n",
    "      testSummary ++= runQuery(queryStr, false, resultDir)\n",
    "    } catch {\n",
    "       case e: Throwable => {\n",
    "            println(\"Error in query \"+ i + \" msg = \" + e.getMessage)\n",
    "            queryErrors += 1\n",
    "       }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  println(\"=====================================================\")\n",
    "  if ( queryErrors > 0) {\n",
    "    println(s\"Query execution failed with $queryErrors errors\")\n",
    "  } else {\n",
    "    println(\"All TPC-DS queries ran successfully\")\n",
    "  }\n",
    "  println (s\"Total Elapsed Time so far: ${totalTime} seconds.\")\n",
    "  println(\"=====================================================\")\n",
    "  testSummary.toDF(\"QueryName\",\"ElapsedTime\",\"RowsReturned\", \"ResultLocation\")\n",
    "}\n",
    "\n",
    "def displaySummary(summaryDF: DataFrame): Unit = {\n",
    "    summaryDF.select(\"QueryName\", \"ElapsedTime\", \"RowsReturned\").show(10000)\n",
    "}\n",
    "\n",
    "def displayResult(queryNum: Int, summaryDF: DataFrame) = {\n",
    "   val queryStr = \"%02d\".format(queryNum)\n",
    "   // Find result files for this query number. For some queries there are\n",
    "   // multiple result files. \n",
    "   val  files = summaryDF.where(s\"queryName like 'query${queryStr}%'\").select(\"ResultLocation\").collect()\n",
    "   for (file <- files) {\n",
    "       val fileName = file.getString(0)\n",
    "       val df = spark.read\n",
    "         .format(\"csv\")\n",
    "         .option(\"header\", \"true\") //reading the headers\n",
    "         .option(\"mode\", \"DROPMALFORMED\")\n",
    "         .load(fileName)\n",
    "       val numRows:Int = df.count().toInt\n",
    "       df.show(numRows, truncate=false)\n",
    "   }\n",
    "}\n",
    "\n",
    "def explainQuery(queryNum: Int) = {\n",
    "  val queryStr = \"%02d\".format(queryNum)  \n",
    "  val queryName = s\"${tpcdsQueriesDir}/query${queryStr}.sql\"   \n",
    "  val (_, content) = spark.sparkContext.wholeTextFiles(queryName).collect()(0)  \n",
    "  val queries = content.split(\"\\n\")\n",
    "    .filterNot (_.startsWith(\"--\"))\n",
    "    .mkString(\" \").split(\";\")\n",
    "    \n",
    "  for (query <- queries)  {    \n",
    "    spark.sql(query).explain(true) \n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the TPC-DS schema\n",
    "* Create the database as specified by tpcdsDatabaseName\n",
    "* Create all the TPC-DS tables\n",
    "* Load the data into the tables in parquet format. Since the data generated by tpc-ds toolkit is in CSV format, we do the loading in multi steps.\n",
    "  * Step 1: we create tables in csv format by pointing the location to the generated data\n",
    "  * Step 2: we create parquet tables by using CTAS to convert text data into parquet format\n",
    "  * Step 3: we drop the text based tables as we longer need them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table call_center ..\n",
      "Creating table catalog_sales ..\n",
      "Creating table customer_demographics ..\n",
      "Creating table income_band ..\n",
      "Creating table promotion ..\n",
      "Creating table store ..\n",
      "Creating table time_dim ..\n",
      "Creating table web_returns ..\n",
      "Creating table catalog_page ..\n",
      "Creating table customer ..\n",
      "Creating table date_dim ..\n",
      "Creating table inventory ..\n",
      "Creating table reason ..\n",
      "Creating table store_returns ..\n",
      "Creating table warehouse ..\n",
      "Creating table web_sales ..\n",
      "Creating table catalog_returns ..\n",
      "Creating table customer_address ..\n",
      "Creating table household_demographics ..\n",
      "Creating table item ..\n",
      "Creating table ship_mode ..\n",
      "Creating table store_sales ..\n",
      "Creating table web_page ..\n",
      "Creating table web_site ..\n"
     ]
    }
   ],
   "source": [
    "// TPC-DS table names.\n",
    "val tables = Array(\"call_center\", \"catalog_sales\",\n",
    "                   \"customer_demographics\", \"income_band\",\n",
    "                   \"promotion\", \"store\", \"time_dim\", \"web_returns\",\n",
    "                   \"catalog_page\", \"customer\", \"date_dim\",\n",
    "                   \"inventory\", \"reason\", \"store_returns\", \"warehouse\",\n",
    "                   \"web_sales\", \"catalog_returns\", \"customer_address\",\n",
    "                   \"household_demographics\", \"item\", \"ship_mode\", \"store_sales\",\n",
    "                   \"web_page\", \"web_site\" )\n",
    "\n",
    "// Create database\n",
    "createDatabase\n",
    "\n",
    "// Create table\n",
    "forEachTable(tables, table => createTable(table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify table creation and data loading.\n",
    "* Run a simple Spark SQL query to get the count of rows\n",
    "* Verify that the row counts are as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Loaded and verified the table counts successfully\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "// Run a count query and get the counts\n",
    "val rowCounts = tables.map { table =>\n",
    "    spark.table(table).count()\n",
    "}\n",
    "\n",
    "val expectedCounts = Array (\n",
    "    6, 1441548, 1920800, 20, 300, 12, 86400,\n",
    "    71763,  11718, 100000, 73049, 11745000, \n",
    "    35, 287514, 5, 719384, 144067, 50000, 7200,\n",
    "    18000, 20, 2880404, 60, 30\n",
    ")\n",
    "\n",
    "var errorCount = 0;\n",
    "val zippedCountsWithIndex = rowCounts.zip(expectedCounts).zipWithIndex\n",
    "for ((pair, index) <- zippedCountsWithIndex) {\n",
    "    if (pair._1 != pair._2) {\n",
    "        println(s\"\"\"ERROR!! Row counts for ${tables(index)} does not match.\n",
    "        Expected=${expectedCounts(index)} but found ${rowCounts(index)}\"\"\")\n",
    "        errorCount += 1\n",
    "    }\n",
    "}\n",
    "\n",
    "println(\"=====================================================\")\n",
    "if ( errorCount > 0) {\n",
    "  println(s\"Load verification failed with $errorCount errors\")\n",
    "} else {\n",
    "  println(\"Loaded and verified the table counts successfully\")\n",
    "}\n",
    "println(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run a single query\n",
    "* Run a query given a query number between 1 to 99\n",
    "* Display the query results, the elapsed time to execute the query and the number of rows returned for the query\n",
    "* To run a different query, please change the QUERY_NUM to a valid value from 1 to 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TPC-DS Query : 01\n",
      "+---------+-----------+------------+\n",
      "|QueryName|ElapsedTime|RowsReturned|\n",
      "+---------+-----------+------------+\n",
      "|  query01|       12.0|         100|\n",
      "+---------+-----------+------------+\n",
      "\n",
      "+----------------+\n",
      "|c_customer_id   |\n",
      "+----------------+\n",
      "|AAAAAAAAAAABBAAA|\n",
      "|AAAAAAAAAAADBAAA|\n",
      "|AAAAAAAAAAADBAAA|\n",
      "|AAAAAAAAAAAKAAAA|\n",
      "|AAAAAAAAAABDAAAA|\n",
      "|AAAAAAAAAABHBAAA|\n",
      "|AAAAAAAAAABLAAAA|\n",
      "|AAAAAAAAAABMAAAA|\n",
      "|AAAAAAAAAACHAAAA|\n",
      "|AAAAAAAAAACMAAAA|\n",
      "|AAAAAAAAAADDAAAA|\n",
      "|AAAAAAAAAADGAAAA|\n",
      "|AAAAAAAAAADGBAAA|\n",
      "|AAAAAAAAAADGBAAA|\n",
      "|AAAAAAAAAADPAAAA|\n",
      "|AAAAAAAAAAEBAAAA|\n",
      "|AAAAAAAAAAEFBAAA|\n",
      "|AAAAAAAAAAEGBAAA|\n",
      "|AAAAAAAAAAEIAAAA|\n",
      "|AAAAAAAAAAEMAAAA|\n",
      "|AAAAAAAAAAFAAAAA|\n",
      "|AAAAAAAAAAFPAAAA|\n",
      "|AAAAAAAAAAGGBAAA|\n",
      "|AAAAAAAAAAGHBAAA|\n",
      "|AAAAAAAAAAGJAAAA|\n",
      "|AAAAAAAAAAGMAAAA|\n",
      "|AAAAAAAAAAHEBAAA|\n",
      "|AAAAAAAAAAHFBAAA|\n",
      "|AAAAAAAAAAIEBAAA|\n",
      "|AAAAAAAAAAJGBAAA|\n",
      "|AAAAAAAAAAJHBAAA|\n",
      "|AAAAAAAAAAKCAAAA|\n",
      "|AAAAAAAAAAKCAAAA|\n",
      "|AAAAAAAAAAKJAAAA|\n",
      "|AAAAAAAAAAKMAAAA|\n",
      "|AAAAAAAAAAKMAAAA|\n",
      "|AAAAAAAAAALAAAAA|\n",
      "|AAAAAAAAAALABAAA|\n",
      "|AAAAAAAAAALGAAAA|\n",
      "|AAAAAAAAAALHBAAA|\n",
      "|AAAAAAAAAALJAAAA|\n",
      "|AAAAAAAAAANHAAAA|\n",
      "|AAAAAAAAAANHBAAA|\n",
      "|AAAAAAAAAANJAAAA|\n",
      "|AAAAAAAAAANMAAAA|\n",
      "|AAAAAAAAAANMAAAA|\n",
      "|AAAAAAAAAANNAAAA|\n",
      "|AAAAAAAAAAOBBAAA|\n",
      "|AAAAAAAAAAODBAAA|\n",
      "|AAAAAAAAAAOLAAAA|\n",
      "|AAAAAAAAAAPGBAAA|\n",
      "|AAAAAAAAABAAAAAA|\n",
      "|AAAAAAAAABAEAAAA|\n",
      "|AAAAAAAAABAEBAAA|\n",
      "|AAAAAAAAABAFBAAA|\n",
      "|AAAAAAAAABAIAAAA|\n",
      "|AAAAAAAAABAOAAAA|\n",
      "|AAAAAAAAABBDBAAA|\n",
      "|AAAAAAAAABCFAAAA|\n",
      "|AAAAAAAAABCHBAAA|\n",
      "|AAAAAAAAABDHAAAA|\n",
      "|AAAAAAAAABENAAAA|\n",
      "|AAAAAAAAABFEBAAA|\n",
      "|AAAAAAAAABFGAAAA|\n",
      "|AAAAAAAAABFMAAAA|\n",
      "|AAAAAAAAABFPAAAA|\n",
      "|AAAAAAAAABGFAAAA|\n",
      "|AAAAAAAAABGFBAAA|\n",
      "|AAAAAAAAABGJAAAA|\n",
      "|AAAAAAAAABIBBAAA|\n",
      "|AAAAAAAAABICBAAA|\n",
      "|AAAAAAAAABIIAAAA|\n",
      "|AAAAAAAAABJNAAAA|\n",
      "|AAAAAAAAABKGBAAA|\n",
      "|AAAAAAAAABLOAAAA|\n",
      "|AAAAAAAAABLPAAAA|\n",
      "|AAAAAAAAABMABAAA|\n",
      "|AAAAAAAAABMPAAAA|\n",
      "|AAAAAAAAABNAAAAA|\n",
      "|AAAAAAAAABNCBAAA|\n",
      "|AAAAAAAAABNEBAAA|\n",
      "|AAAAAAAAABNLAAAA|\n",
      "|AAAAAAAAABNOAAAA|\n",
      "|AAAAAAAAABNPAAAA|\n",
      "|AAAAAAAAABOAAAAA|\n",
      "|AAAAAAAAABOFBAAA|\n",
      "|AAAAAAAAABOOAAAA|\n",
      "|AAAAAAAAABOPAAAA|\n",
      "|AAAAAAAAABPEAAAA|\n",
      "|AAAAAAAAACADAAAA|\n",
      "|AAAAAAAAACAFAAAA|\n",
      "|AAAAAAAAACAFAAAA|\n",
      "|AAAAAAAAACAHBAAA|\n",
      "|AAAAAAAAACAJAAAA|\n",
      "|AAAAAAAAACBDAAAA|\n",
      "|AAAAAAAAACBDAAAA|\n",
      "|AAAAAAAAACBEBAAA|\n",
      "|AAAAAAAAACBNAAAA|\n",
      "|AAAAAAAAACBPAAAA|\n",
      "|AAAAAAAAACCHAAAA|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val QUERY_NUM = 1\n",
    "val result = runIndividualQuery(QUERY_NUM)\n",
    "displaySummary(result)\n",
    "displayResult(QUERY_NUM, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all the TPC-DS queries\n",
    "* Runs all the queries starting from 1 to 99\n",
    "* The query results are saved and can be queried by calling getResults method.\n",
    "* The summary will be shown at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TPC-DS Query : 01\n",
      "Running TPC-DS Query : 02\n",
      "Running TPC-DS Query : 03\n",
      "Running TPC-DS Query : 04\n",
      "Running TPC-DS Query : 05\n",
      "Running TPC-DS Query : 06\n",
      "Running TPC-DS Query : 07\n",
      "Running TPC-DS Query : 08\n",
      "Running TPC-DS Query : 09\n",
      "Running TPC-DS Query : 10\n",
      "Running TPC-DS Query : 11\n",
      "Running TPC-DS Query : 12\n",
      "Running TPC-DS Query : 13\n",
      "Running TPC-DS Query : 14\n",
      "Running TPC-DS Query : 15\n",
      "Running TPC-DS Query : 16\n",
      "Running TPC-DS Query : 17\n",
      "Running TPC-DS Query : 18\n",
      "Running TPC-DS Query : 19\n",
      "Running TPC-DS Query : 20\n",
      "Running TPC-DS Query : 21\n",
      "Running TPC-DS Query : 22\n",
      "Running TPC-DS Query : 23\n",
      "Running TPC-DS Query : 24\n",
      "Running TPC-DS Query : 25\n",
      "Running TPC-DS Query : 26\n",
      "Running TPC-DS Query : 27\n",
      "Running TPC-DS Query : 28\n",
      "Running TPC-DS Query : 29\n",
      "Running TPC-DS Query : 30\n",
      "Running TPC-DS Query : 31\n",
      "Running TPC-DS Query : 32\n",
      "Running TPC-DS Query : 33\n",
      "Running TPC-DS Query : 34\n",
      "Running TPC-DS Query : 35\n",
      "Running TPC-DS Query : 36\n",
      "Running TPC-DS Query : 37\n",
      "Running TPC-DS Query : 38\n",
      "Running TPC-DS Query : 39\n",
      "Running TPC-DS Query : 40\n",
      "Running TPC-DS Query : 41\n",
      "Running TPC-DS Query : 42\n",
      "Running TPC-DS Query : 43\n",
      "Running TPC-DS Query : 44\n",
      "Running TPC-DS Query : 45\n",
      "Running TPC-DS Query : 46\n",
      "Running TPC-DS Query : 47\n",
      "Running TPC-DS Query : 48\n",
      "Running TPC-DS Query : 49\n",
      "Running TPC-DS Query : 50\n",
      "Running TPC-DS Query : 51\n",
      "Running TPC-DS Query : 52\n",
      "Running TPC-DS Query : 53\n",
      "Running TPC-DS Query : 54\n",
      "Running TPC-DS Query : 55\n",
      "Running TPC-DS Query : 56\n",
      "Running TPC-DS Query : 57\n",
      "Running TPC-DS Query : 58\n",
      "Running TPC-DS Query : 59\n",
      "Running TPC-DS Query : 60\n",
      "Running TPC-DS Query : 61\n",
      "Running TPC-DS Query : 62\n",
      "Running TPC-DS Query : 63\n",
      "Running TPC-DS Query : 64\n",
      "Running TPC-DS Query : 65\n",
      "Running TPC-DS Query : 66\n",
      "Running TPC-DS Query : 67\n",
      "Running TPC-DS Query : 68\n",
      "Running TPC-DS Query : 69\n",
      "Running TPC-DS Query : 70\n",
      "Running TPC-DS Query : 71\n",
      "Running TPC-DS Query : 72\n",
      "Running TPC-DS Query : 73\n",
      "Running TPC-DS Query : 74\n",
      "Running TPC-DS Query : 75\n",
      "Running TPC-DS Query : 76\n",
      "Running TPC-DS Query : 77\n",
      "Running TPC-DS Query : 78\n",
      "Running TPC-DS Query : 79\n",
      "Running TPC-DS Query : 80\n",
      "Running TPC-DS Query : 81\n",
      "Running TPC-DS Query : 82\n",
      "Running TPC-DS Query : 83\n",
      "Running TPC-DS Query : 84\n",
      "Running TPC-DS Query : 85\n",
      "Running TPC-DS Query : 86\n",
      "Running TPC-DS Query : 87\n",
      "Running TPC-DS Query : 88\n",
      "Running TPC-DS Query : 89\n",
      "Running TPC-DS Query : 90\n",
      "Running TPC-DS Query : 91\n",
      "Running TPC-DS Query : 92\n",
      "Running TPC-DS Query : 93\n",
      "Running TPC-DS Query : 94\n",
      "Running TPC-DS Query : 95\n",
      "Running TPC-DS Query : 96\n",
      "Running TPC-DS Query : 97\n",
      "Running TPC-DS Query : 98\n",
      "Running TPC-DS Query : 99\n",
      "=====================================================\n",
      "All TPC-DS queries ran successfully\n",
      "Total Elapsed Time so far: 1276 seconds.\n",
      "=====================================================\n",
      "+---------+-----------+------------+\n",
      "|QueryName|ElapsedTime|RowsReturned|\n",
      "+---------+-----------+------------+\n",
      "|  query01|        7.0|         100|\n",
      "|  query02|        6.0|        2513|\n",
      "|  query03|        6.0|          89|\n",
      "|  query04|       41.0|           8|\n",
      "|  query05|        9.0|         100|\n",
      "|  query06|       18.0|          45|\n",
      "|  query07|        6.0|         100|\n",
      "|  query08|        8.0|           5|\n",
      "|  query09|        1.0|           1|\n",
      "|  query10|       10.0|           5|\n",
      "|  query11|       25.0|          88|\n",
      "|  query12|       10.0|         100|\n",
      "|  query13|        8.0|           1|\n",
      "|query14-1|       47.0|         100|\n",
      "|query14-2|       52.0|         100|\n",
      "|  query15|        8.0|         100|\n",
      "|  query16|       15.0|           1|\n",
      "|  query17|        8.0|           1|\n",
      "|  query18|        7.0|         100|\n",
      "|  query19|        4.0|         100|\n",
      "|  query20|        5.0|         100|\n",
      "|  query21|        9.0|         100|\n",
      "|  query22|        8.0|         100|\n",
      "|query23-1|       27.0|           1|\n",
      "|query23-2|       29.0|           4|\n",
      "|query24-1|       11.0|           0|\n",
      "|query24-2|        9.0|           0|\n",
      "|  query25|        6.0|           1|\n",
      "|  query26|        3.0|         100|\n",
      "|  query27|        4.0|         100|\n",
      "|  query28|        4.0|           1|\n",
      "|  query29|        8.0|           1|\n",
      "|  query30|        8.0|         100|\n",
      "|  query31|       13.0|          51|\n",
      "|  query32|        4.0|           1|\n",
      "|  query33|        8.0|         100|\n",
      "|  query34|        5.0|         451|\n",
      "|  query35|       10.0|         100|\n",
      "|  query36|        7.0|         100|\n",
      "|  query37|        2.0|           1|\n",
      "|  query38|       15.0|           1|\n",
      "|query39-1|       13.0|         246|\n",
      "|query39-2|       18.0|          17|\n",
      "|  query40|        7.0|         100|\n",
      "|  query41|        1.0|           4|\n",
      "|  query42|        2.0|          10|\n",
      "|  query43|        2.0|           6|\n",
      "|  query44|        3.0|          10|\n",
      "|  query45|       12.0|          19|\n",
      "|  query46|        6.0|         100|\n",
      "|  query47|       38.0|         100|\n",
      "|  query48|        6.0|           1|\n",
      "|  query49|        4.0|          32|\n",
      "|  query50|        8.0|           6|\n",
      "|  query51|       37.0|         100|\n",
      "|  query52|        3.0|         100|\n",
      "|  query53|        9.0|         100|\n",
      "|  query54|        6.0|           1|\n",
      "|  query55|        2.0|         100|\n",
      "|  query56|        5.0|         100|\n",
      "|  query57|       26.0|         100|\n",
      "|  query58|       14.0|           3|\n",
      "|  query59|        4.0|         100|\n",
      "|  query60|       11.0|         100|\n",
      "|  query61|       16.0|           1|\n",
      "|  query62|        7.0|         100|\n",
      "|  query63|        9.0|         100|\n",
      "|  query64|       70.0|          10|\n",
      "|  query65|        8.0|         100|\n",
      "|  query66|        7.0|           5|\n",
      "|  query67|       14.0|         100|\n",
      "|  query68|        5.0|         100|\n",
      "|  query69|        6.0|         100|\n",
      "|  query70|        4.0|           3|\n",
      "|  query71|        6.0|        1018|\n",
      "|  query72|       51.0|         100|\n",
      "|  query73|        3.0|           5|\n",
      "|  query74|       31.0|          92|\n",
      "|  query75|       60.0|         100|\n",
      "|  query76|        6.0|         100|\n",
      "|  query77|       42.0|          44|\n",
      "|  query78|       27.0|         100|\n",
      "|  query79|        5.0|         100|\n",
      "|  query80|       24.0|         100|\n",
      "|  query81|       12.0|         100|\n",
      "|  query82|        2.0|           2|\n",
      "|  query83|       12.0|          21|\n",
      "|  query84|        2.0|          25|\n",
      "|  query85|        5.0|           6|\n",
      "|  query86|        4.0|         100|\n",
      "|  query87|       12.0|           1|\n",
      "|  query88|        9.0|           1|\n",
      "|  query89|       12.0|         100|\n",
      "|  query90|        3.0|           1|\n",
      "|  query91|        5.0|           1|\n",
      "|  query92|        3.0|           1|\n",
      "|  query93|        3.0|         100|\n",
      "|  query94|        9.0|           1|\n",
      "|  query95|       12.0|           1|\n",
      "|  query96|        4.0|           1|\n",
      "|  query97|        9.0|           1|\n",
      "|  query98|       12.0|        2516|\n",
      "|  query99|        5.0|          90|\n",
      "+---------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val result = runAllQueries()\n",
    "displaySummary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Result for a individual Query\n",
    "* Reads the result file for the given query stored when thery are run in previous steps.\n",
    "* Certain queries have multiple associated result files. The result files are read in sequence and\n",
    "  results are displayed.\n",
    "* If the result file(s) are not found , then an error is displayed.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "displayResult(1, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display SQL Execution Plan\n",
    "* Display the analyzed, optimized and phyical plan for a given query.\n",
    "* Can be used by developers for debugging purposes.\n",
    "* QUERY_NUM can be changed to display the plan for different query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val QUERY_NUM=1\n",
    "explainQuery(QUERY_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "         <link rel=\"stylesheet\" type=\"text/css\" href=\"https://brunelvis.org/js/brunel.2.4.css\" charset=\"utf-8\">\n",
       "         <link rel=\"stylesheet\" type=\"text/css\" href=\"https://brunelvis.org/js/sumoselect.css\" charset=\"utf-8\">\n",
       "         <style>  </style>\n",
       "         <div id=\"controlsIddc853c47-9897-4eca-9c9d-b95af4fa983e\" class=\"brunel\"/>\n",
       "<svg id=\"visida9253fda-f1f6-460d-a54f-62cd91201ede\" width=\"500\" height=\"400\"></svg>\n",
       "\n",
       "<script>\n",
       "require.config({\n",
       "            waitSeconds: 60,\n",
       "            paths: {\n",
       "                'd3': '//cdnjs.cloudflare.com/ajax/libs/d3/4.2.1/d3.min',\n",
       "                'topojson' : '//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.20/topojson.min',\n",
       "                'brunel' : 'https://brunelvis.org/js/brunel.2.4.min',\n",
       "                'brunelControls' : 'https://brunelvis.org/js/brunel.controls.2.4.min'\n",
       "            },\n",
       "\n",
       "            shim: {\n",
       "               'brunel' : {\n",
       "                    exports: 'BrunelD3',\n",
       "                    deps: ['d3', 'topojson'],\n",
       "                    init: function() {\n",
       "                       return {\n",
       "                         BrunelD3 : BrunelD3,\n",
       "                         BrunelData : BrunelData\n",
       "                      }\n",
       "                    }\n",
       "                },\n",
       "               'brunelControls' : {\n",
       "                    exports: 'BrunelEventHandlers',\n",
       "                    init: function() {\n",
       "                       return {\n",
       "                         BrunelEventHandlers: BrunelEventHandlers,\n",
       "                         BrunelJQueryControlFactory: BrunelJQueryControlFactory\n",
       "                      }\n",
       "                    }\n",
       "                }\n",
       "\n",
       "            }\n",
       "\n",
       "        });\n",
       "\n",
       "        require([\"d3\"], function(d3) {\n",
       "        require([\"brunel\", \"brunelControls\"], function(brunel, brunelControls) {\n",
       "\n",
       "            function  BrunelVis(visId) {\n",
       "  \"use strict\";                                                                       // strict mode\n",
       "  var datasets = [],                                      // array of datasets for the original data\n",
       "      pre = function(d, i) { return d },                         // default pre-process does nothing\n",
       "      post = function(d, i) { return d },                       // default post-process does nothing\n",
       "      transitionTime = 200,                                        // transition time for animations\n",
       "      charts = [],                                                       // the charts in the system\n",
       "      vis = d3.select('#' + visId).attr('class', 'brunel');                     // the SVG container\n",
       "\n",
       "  BrunelD3.addDefinitions(vis);                                   // ensure standard symbols present\n",
       "\n",
       "  // Define chart #1 in the visualization //////////////////////////////////////////////////////////\n",
       "\n",
       "  charts[0] = function(parentNode, filterRows) {\n",
       "    var geom = BrunelD3.geometry(parentNode || vis.node(), 0, 0, 1, 1, 22, 43, 89, 8),\n",
       "      elements = [];                                              // array of elements in this chart\n",
       "\n",
       "    // Define groups for the chart parts ///////////////////////////////////////////////////////////\n",
       "\n",
       "    var chart =  vis.append('g').attr('class', 'chart1')\n",
       "      .attr('transform','translate(' + geom.chart_left + ',' + geom.chart_top + ')');\n",
       "    var overlay = chart.append('g').attr('class', 'element').attr('class', 'overlay');\n",
       "    var zoom = d3.zoom().scaleExtent([1/3,3]);\n",
       "    var zoomNode = overlay.append('rect').attr('class', 'overlay')\n",
       "      .attr('x', geom.inner_left).attr('y', geom.inner_top)\n",
       "      .attr('width', geom.inner_rawWidth).attr('height', geom.inner_rawHeight)\n",
       "      .style('cursor', 'move').call(zoom)\n",
       "      .node();\n",
       "    zoomNode.__zoom = d3.zoomIdentity;\n",
       "    chart.append('rect').attr('class', 'background').attr('width', geom.chart_right-geom.chart_left).attr('height', geom.chart_bottom-geom.chart_top);\n",
       "    var interior = chart.append('g').attr('class', 'interior zoomNone')\n",
       "      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')')\n",
       "      .attr('clip-path', 'url(#clip_visida9253fda-f1f6-460d-a54f-62cd91201ede_chart1_inner)');\n",
       "    interior.append('rect').attr('class', 'inner').attr('width', geom.inner_width).attr('height', geom.inner_height);\n",
       "    var gridGroup = interior.append('g').attr('class', 'grid');\n",
       "    var axes = chart.append('g').attr('class', 'axis')\n",
       "      .attr('transform','translate(' + geom.inner_left + ',' + geom.inner_top + ')');\n",
       "    vis.select('defs').append('clipPath').attr('id', 'clip_visida9253fda-f1f6-460d-a54f-62cd91201ede_chart1_inner').append('rect')\n",
       "      .attr('x', 0).attr('y', 0)\n",
       "      .attr('width', geom.inner_rawWidth+1).attr('height', geom.inner_rawHeight+1);\n",
       "    chart.append('text').attr('class', 'title header').text('Query Execution Time in seconds').style('text-anchor', 'middle')\n",
       "      .attr('x','50%')\n",
       "      .attr('y',2).attr('dy','0.8em');\n",
       "    chart.append('text').attr('class', 'title footer').text('Execution Summary').style('text-anchor', 'end')\n",
       "      .attr('x','100%')\n",
       "      .attr('dx', -2)\n",
       "      .attr('y','100%').attr('dy', -6);\n",
       "\n",
       "    // Scales //////////////////////////////////////////////////////////////////////////////////////\n",
       "\n",
       "    var scale_x = d3.scalePoint().padding(0.5)\n",
       "      .domain(['query01', 'query02', 'query03', 'query04', 'query05', 'query06', 'query07', 'query08', 'query09', 'query10', 'query11', 'query12', 'query13', 'query14-1', 'query14-2', 'query15', 'query16', 'query17', 'query18', 'query19', 'query20', 'query21', 'query22', 'query23-1', 'query23-2', 'query24-1', 'query24-2', 'query25', 'query26', 'query27', 'query28', 'query29', 'query30', 'query31', 'query32', 'query33', 'query34', 'query35', 'query36', 'query37', 'query38', 'query39-1', 'query39-2', 'query40', 'query41', 'query42', 'query43', 'query44', 'query45', 'query46', 'query47', 'query48', 'query49', 'query50', 'query51', 'query52', 'query53', 'query54', 'query55', 'query56', 'query57', 'query58', 'query59', 'query60', 'query61', 'query62', 'query63', 'query64', 'query65', 'query66', 'query67', 'query68', 'query69', 'query70', 'query71', 'query72', 'query73', 'query74', 'query75', 'query76', 'query77', 'query78', 'query79', 'query80', 'query81', 'query82', 'query83', 'query84', 'query85', 'query86', 'query87', 'query88', 'query89', 'query90', 'query91', 'query92', 'query93', 'query94', 'query95', 'query96', 'query97', 'query98', 'query99'])\n",
       "      .range([0, geom.inner_width]);\n",
       "    var scale_inner = d3.scaleLinear().domain([0,1])\n",
       "      .range([-0.5, 0.5]);\n",
       "    var scale_y = d3.scaleSqrt().domain([0, 80.000008])\n",
       "      .range([geom.inner_height, 0]);\n",
       "    var base_scales = [scale_x, scale_y];                           // untransformed original scales\n",
       "\n",
       "    // Axes ////////////////////////////////////////////////////////////////////////////////////////\n",
       "\n",
       "    axes.append('g').attr('class', 'x axis')\n",
       "      .attr('transform','translate(0,' + geom.inner_rawHeight + ')')\n",
       "      .attr('clip-path', 'url(#clip_visida9253fda-f1f6-460d-a54f-62cd91201ede_chart1_haxis)');\n",
       "    vis.select('defs').append('clipPath').attr('id', 'clip_visida9253fda-f1f6-460d-a54f-62cd91201ede_chart1_haxis').append('polyline')\n",
       "      .attr('points', '-1,-1000, -1,-1 -5,5, -1000,5, -100,1000, 10000,1000 10000,-1000');\n",
       "    axes.select('g.axis.x').append('text').attr('class', 'title').text('QueryName').style('text-anchor', 'middle')\n",
       "      .attr('x',geom.inner_rawWidth/2)\n",
       "      .attr('y', geom.inner_bottom - 18.0).attr('dy','-0.27em');\n",
       "    axes.append('g').attr('class', 'y axis')\n",
       "      .attr('clip-path', 'url(#clip_visida9253fda-f1f6-460d-a54f-62cd91201ede_chart1_vaxis)');\n",
       "    vis.select('defs').append('clipPath').attr('id', 'clip_visida9253fda-f1f6-460d-a54f-62cd91201ede_chart1_vaxis').append('polyline')\n",
       "      .attr('points', '-1000,-10000, 10000,-10000, 10000,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+1) + ', -1,' + (geom.inner_rawHeight+5) + ', -1000,' + (geom.inner_rawHeight+5) );\n",
       "    axes.select('g.axis.y').append('text').attr('class', 'title').text('ElapsedTime').style('text-anchor', 'middle')\n",
       "      .attr('x',-geom.inner_rawHeight/2)\n",
       "      .attr('y', 4-geom.inner_left).attr('dy', '0.7em').attr('transform', 'rotate(270)');\n",
       "\n",
       "    var axis_bottom = d3.axisBottom(scale_x).ticks(Math.min(10, Math.round(geom.inner_rawWidth / 82.5)));\n",
       "    var axis_left = d3.axisLeft(scale_y).ticks(Math.min(10, Math.round(geom.inner_rawHeight / 20)));\n",
       "\n",
       "    function buildAxes(time) {\n",
       "      axis_bottom.tickValues(BrunelD3.filterTicks(scale_x))\n",
       "      var axis_x = axes.select('g.axis.x');\n",
       "      BrunelD3.transition(axis_x, time).call(axis_bottom.scale(scale_x)).selectAll('.tick text')\n",
       "        .attr('transform', function() {\n",
       "          var v = this.getComputedTextLength() / Math.sqrt(2)/2;\n",
       "          return 'translate(-' + (v+6) + ',' + v + ') rotate(-45)'\n",
       "      });\n",
       "      var axis_y = axes.select('g.axis.y');\n",
       "      BrunelD3.transition(axis_y, time).call(axis_left.scale(scale_y));\n",
       "    }\n",
       "    zoom.on('zoom', function(t, time) {\n",
       "        t = t ||BrunelD3.restrictZoom(d3.event.transform, geom, this);\n",
       "        scale_y = t.rescaleY(base_scales[1]);\n",
       "        zoomNode.__zoom = t;\n",
       "        interior.attr('class', 'interior ' + BrunelD3.zoomLabel(t.k));;\n",
       "        build(time || -1);\n",
       "    });\n",
       "\n",
       "    // Define element #1 ///////////////////////////////////////////////////////////////////////////\n",
       "\n",
       "    elements[0] = function() {\n",
       "      var original, processed,                           // data sets passed in and then transformed\n",
       "        element, data,                                 // brunel element information and brunel data\n",
       "        selection, merged;                                      // d3 selection and merged selection\n",
       "      var elementGroup = interior.append('g').attr('class', 'element1'),\n",
       "        main = elementGroup.append('g').attr('class', 'main'),\n",
       "        labels = BrunelD3.undoTransform(elementGroup.append('g').attr('class', 'labels').attr('aria-hidden', 'true'), elementGroup);\n",
       "\n",
       "      function makeData() {\n",
       "        original = datasets[0];\n",
       "        if (filterRows) original = original.retainRows(filterRows);\n",
       "        processed = pre(original, 0);\n",
       "        processed = post(processed, 0);\n",
       "        var f0 = processed.field('QueryName'),\n",
       "          f1 = processed.field('ElapsedTime'),\n",
       "          f2 = processed.field('#row'),\n",
       "          f3 = processed.field('#selection');\n",
       "        var keyFunc = function(d) { return f0.value(d) };\n",
       "        data = {\n",
       "          QueryName:    function(d) { return f0.value(d.row) },\n",
       "          ElapsedTime:  function(d) { return f1.value(d.row) },\n",
       "          $row:         function(d) { return f2.value(d.row) },\n",
       "          $selection:   function(d) { return f3.value(d.row) },\n",
       "          QueryName_f:  function(d) { return f0.valueFormatted(d.row) },\n",
       "          ElapsedTime_f:function(d) { return f1.valueFormatted(d.row) },\n",
       "          $row_f:       function(d) { return f2.valueFormatted(d.row) },\n",
       "          $selection_f: function(d) { return f3.valueFormatted(d.row) },\n",
       "          _split:       function(d) { return 'ALL' },\n",
       "          _key:         keyFunc,\n",
       "          _rows:        BrunelD3.makeRowsWithKeys(keyFunc, processed.rowCount())\n",
       "        };\n",
       "      }\n",
       "\n",
       "      // Build element from data ///////////////////////////////////////////////////////////////////\n",
       "\n",
       "      function build(transitionMillis) {\n",
       "        element = elements[0];\n",
       "        var w = 0.9 * Math.abs(scale_x(scale_x.domain()[1]) - scale_x(scale_x.domain()[0]) );\n",
       "        var x = function(d) { return scale_x(data.QueryName(d))};\n",
       "        var h = geom.default_point_size;\n",
       "        var y1 = scale_y.range()[0];\n",
       "        var y2 = function(d) { return scale_y(data.ElapsedTime(d))};\n",
       "\n",
       "        // Define selection entry operations\n",
       "        function initialState(selection) {\n",
       "          selection\n",
       "            .attr('class', 'element bar filled')\n",
       "            .style('pointer-events', 'none')\n",
       "            .each(function(d) {\n",
       "              var width = w, left = x(d) - width/2, \n",
       "              c = y1, d = y2(d), top = Math.min(c,d), height = Math.max(1e-6, Math.abs(c-d));\n",
       "              this.r = {x:left, y:top, w:width, h:height};\n",
       "            })\n",
       "            .attr('x', function(d) { return this.r.x })\n",
       "            .attr('y', function(d) { return this.r.y + this.r.h/2 })\n",
       "            .attr('width', function(d) { return this.r.w })\n",
       "            .attr('height',0)\n",
       "        }\n",
       "\n",
       "        // Define selection update operations on merged data\n",
       "        function updateState(selection) {\n",
       "          selection\n",
       "            .each(function(d) {\n",
       "              var width = w, left = x(d) - width/2, \n",
       "              c = y1, d = y2(d), top = Math.min(c,d), height = Math.max(1e-6, Math.abs(c-d));\n",
       "              this.r = {x:left, y:top, w:width, h:height};\n",
       "            })\n",
       "            .attr('x', function(d) { return this.r.x })\n",
       "            .attr('y', function(d) { return this.r.y })\n",
       "            .attr('width', function(d) { return this.r.w })\n",
       "            .attr('height', function(d) { return this.r.h });\n",
       "        }\n",
       "\n",
       "        // Define labeling for the selection\n",
       "        function label(selection, transitionMillis) {\n",
       "        }\n",
       "        // Create selections, set the initial state and transition updates\n",
       "        selection = main.selectAll('.element').data(data._rows, function(d) { return d.key });\n",
       "        var added = selection.enter().append('rect');\n",
       "        merged = selection.merge(added);\n",
       "        initialState(added);\n",
       "        selection.filter(BrunelD3.hasData)\n",
       "          .classed('selected', BrunelD3.isSelected(data))\n",
       "          .filter(BrunelD3.isSelected(data)).raise();\n",
       "        updateState(BrunelD3.transition(merged, transitionMillis));\n",
       "\n",
       "        selection.exit().each(function() { this.remove(); BrunelD3.removeLabels(this)} );\n",
       "      }\n",
       "\n",
       "      return {\n",
       "        data:           function() { return processed },\n",
       "        original:       function() { return original },\n",
       "        internal:       function() { return data },\n",
       "        selection:      function() { return merged },\n",
       "        makeData:       makeData,\n",
       "        build:          build,\n",
       "        chart:          function() { return charts[0] },\n",
       "        group:          function() { return elementGroup },\n",
       "        fields: {\n",
       "          x:            ['QueryName'],\n",
       "          y:            ['ElapsedTime'],\n",
       "          key:          ['QueryName']\n",
       "        }\n",
       "      };\n",
       "    }();\n",
       "\n",
       "    function build(time, noData) {\n",
       "      var first = elements[0].data() == null;\n",
       "      if (first) time = 0;                                           // no transition for first call\n",
       "      buildAxes(time);\n",
       "      if ((first || time > -1) && !noData) {\n",
       "        elements[0].makeData();\n",
       "      }\n",
       "      elements[0].build(time);\n",
       "    }\n",
       "\n",
       "    // Expose the following components of the chart\n",
       "    return {\n",
       "      elements : elements,\n",
       "      interior : interior,\n",
       "      scales: {x:scale_x, y:scale_y},\n",
       "      zoom: function(params, time) {\n",
       "          if (params) zoom.on('zoom').call(zoomNode, params, time);\n",
       "          return d3.zoomTransform(zoomNode);\n",
       "      },\n",
       "      build : build\n",
       "    };\n",
       "    }();\n",
       "\n",
       "  function setData(rowData, i) { datasets[i||0] = BrunelD3.makeData(rowData) }\n",
       "  function updateAll(time) { charts.forEach(function(x) {x.build(time || 0)}) }\n",
       "  function buildAll() {\n",
       "    for (var i=0;i<arguments.length;i++) setData(arguments[i], i);\n",
       "    updateAll(transitionTime);\n",
       "  }\n",
       "\n",
       "  return {\n",
       "    dataPreProcess:     function(f) { if (f) pre = f; return pre },\n",
       "    dataPostProcess:    function(f) { if (f) post = f; return post },\n",
       "    data:               function(d,i) { if (d) setData(d,i); return datasets[i||0] },\n",
       "    visId:              visId,\n",
       "    build:              buildAll,\n",
       "    rebuild:            updateAll,\n",
       "    charts:             charts\n",
       "  }\n",
       "}\n",
       "\n",
       "// Data Tables /////////////////////////////////////////////////////////////////////////////////////\n",
       "\n",
       "var table1 = {\n",
       "   summarized: false,\n",
       "   names: ['QueryName', 'ElapsedTime'], \n",
       "   options: ['string', 'numeric'], \n",
       "   rows: [['query01', 7], ['query02', 6], ['query03', 6], ['query04', 41], ['query05', 9],\n",
       "  ['query06', 18], ['query07', 6], ['query08', 8], ['query09', 1], ['query10', 10], ['query11', 25],\n",
       "  ['query12', 10], ['query13', 8], ['query14-1', 47], ['query14-2', 52], ['query15', 8],\n",
       "  ['query16', 15], ['query17', 8], ['query18', 7], ['query19', 4], ['query20', 5], ['query21', 9],\n",
       "  ['query22', 8], ['query23-1', 27], ['query23-2', 29], ['query24-1', 11], ['query24-2', 9],\n",
       "  ['query25', 6], ['query26', 3], ['query27', 4], ['query28', 4], ['query29', 8], ['query30', 8],\n",
       "  ['query31', 13], ['query32', 4], ['query33', 8], ['query34', 5], ['query35', 10], ['query36', 7],\n",
       "  ['query37', 2], ['query38', 15], ['query39-1', 13], ['query39-2', 18], ['query40', 7],\n",
       "  ['query41', 1], ['query42', 2], ['query43', 2], ['query44', 3], ['query45', 12], ['query46', 6],\n",
       "  ['query47', 38], ['query48', 6], ['query49', 4], ['query50', 8], ['query51', 37], ['query52', 3],\n",
       "  ['query53', 9], ['query54', 6], ['query55', 2], ['query56', 5], ['query57', 26], ['query58', 14],\n",
       "  ['query59', 4], ['query60', 11], ['query61', 16], ['query62', 7], ['query63', 9], ['query64', 70],\n",
       "  ['query65', 8], ['query66', 7], ['query67', 14], ['query68', 5], ['query69', 6], ['query70', 4],\n",
       "  ['query71', 6], ['query72', 51], ['query73', 3], ['query74', 31], ['query75', 60], ['query76', 6],\n",
       "  ['query77', 42], ['query78', 27], ['query79', 5], ['query80', 24], ['query81', 12], ['query82', 2],\n",
       "  ['query83', 12], ['query84', 2], ['query85', 5], ['query86', 4], ['query87', 12], ['query88', 9],\n",
       "  ['query89', 12], ['query90', 3], ['query91', 5], ['query92', 3], ['query93', 3], ['query94', 9],\n",
       "  ['query95', 12], ['query96', 4], ['query97', 9], ['query98', 12], ['query99', 5]]\n",
       "};\n",
       "\n",
       "// Call Code to Build the system ///////////////////////////////////////////////////////////////////\n",
       "\n",
       "var v  = new BrunelVis('visida9253fda-f1f6-460d-a54f-62cd91201ede');\n",
       "v.build(table1);\n",
       "\n",
       "            \"\"\n",
       "        });\n",
       "        });\n",
       "        </script>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%brunel\n",
    "data('result') bar x(QueryName) y(ElapsedTime) title(\"Query Execution Time in seconds\", \"Execution Summary\":footer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn more\n",
    "Visit [Apache Spark](https://spark.apache.org) for learning about spark. For questions or requests plese visit [Spark Community](https://spark.apache.org/community.html). To get involved , see [Contributing to Apache Spark](https://spark.apache.org/contributing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "* Dilip Biswal is a Senior Software Engineer at the Spark Technology Center at IBM. He is an active Apache Spark contributor and works in the open source community.\n",
    "  He is experienced in Relational Databases, Distributed Computing and Big Data Analytics.  He has extensively worked on SQL engines like Informix, Derby, and Big SQL.\n",
    "* Sunitha Kambhampati is an Advisory Software Engineer at the Spark Technology Center at IBM. She is an Apache Spark contributor and works in the open source community. She is experienced in Big Data Analytics.\n",
    "* Xin Wu is an Advisory Software Engineer and is an active contributor for Apache Spark. He has experiences in distributed query processing engines like BigSQL, DB2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark 2.1",
   "language": "scala",
   "name": "scala-spark21"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
