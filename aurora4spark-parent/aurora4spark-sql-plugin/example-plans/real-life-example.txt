== Parsed Logical Plan ==
'Sort ['joined_timestamp ASC NULLS FIRST], true
+- 'Aggregate ['userId, 'joined_timestamp], [unresolvedalias('SUM('totalPrice), None), 'userId, 'joined_timestamp]
   +- 'Join Inner, ('Orders.userId = 'Users.id)
      :- 'UnresolvedRelation [Users], [], false
      +- 'UnresolvedRelation [Orders], [], false

== Analyzed Logical Plan ==
sum(CAST(totalPrice AS DOUBLE)): double, userId: string, joined_timestamp: bigint
Project [sum(CAST(totalPrice AS DOUBLE))#35, userId#25, joined_timestamp#3L]
+- Sort [joined_timestamp#3L ASC NULLS FIRST], true
   +- Aggregate [userId#25, joined_timestamp#3L], [sum(cast(totalPrice#27 as double)) AS sum(CAST(totalPrice AS DOUBLE))#35, userId#25, joined_timestamp#3L]
      +- Join Inner, (cast(userId#25 as int) = id#0)
         :- SubqueryAlias users
         :  +- Relation[ID#0,NAME#1,SURNAME#2,JOINED_TIMESTAMP#3L] JDBCRelation(Users) [numPartitions=1]
         +- SubqueryAlias orders
            +- Relation[id#24,userId#25,orderTimestamp#26,totalPrice#27,items#28] csv

== Optimized Logical Plan ==
Sort [joined_timestamp#3L ASC NULLS FIRST], true
+- Aggregate [userId#25, joined_timestamp#3L], [sum(cast(totalPrice#27 as double)) AS sum(CAST(totalPrice AS DOUBLE))#35, userId#25, joined_timestamp#3L]
   +- Project [JOINED_TIMESTAMP#3L, userId#25, totalPrice#27]
      +- Join Inner, (cast(userId#25 as int) = id#0)
         :- Project [ID#0, JOINED_TIMESTAMP#3L]
         :  +- Filter isnotnull(id#0)
         :     +- Relation[ID#0,NAME#1,SURNAME#2,JOINED_TIMESTAMP#3L] JDBCRelation(Users) [numPartitions=1]
         +- Project [userId#25, totalPrice#27]
            +- Filter isnotnull(userId#25)
               +- Relation[id#24,userId#25,orderTimestamp#26,totalPrice#27,items#28] csv

== Physical Plan ==
*(4) Sort [joined_timestamp#3L ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(joined_timestamp#3L ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#83]
   +- *(3) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[sum(cast(totalPrice#27 as double))], output=[sum(CAST(totalPrice AS DOUBLE))#35, userId#25, joined_timestamp#3L])
      +- Exchange hashpartitioning(userId#25, joined_timestamp#3L, 200), ENSURE_REQUIREMENTS, [id=#79]
         +- *(2) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[partial_sum(cast(totalPrice#27 as double))], output=[userId#25, joined_timestamp#3L, sum#41])
            +- *(2) Project [JOINED_TIMESTAMP#3L, userId#25, totalPrice#27]
               +- *(2) BroadcastHashJoin [id#0], [cast(userId#25 as int)], Inner, BuildRight, false
                  :- *(2) Scan JDBCRelation(Users) [numPartitions=1] [ID#0,JOINED_TIMESTAMP#3L] PushedFilters: [*IsNotNull(ID)], ReadSchema: struct<ID:int,JOINED_TIMESTAMP:bigint>
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, string, false] as int) as bigint)),false), [id=#73]
                     +- *(1) Filter isnotnull(userId#25)
                        +- FileScan csv [userId#25,totalPrice#27] Batched: false, DataFilters: [isnotnull(userId#25)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/William/IdeaProjects/aurora4spark/aurora4spark-parent/aurora4spa..., PartitionFilters: [], PushedFilters: [IsNotNull(userId)], ReadSchema: struct<userId:string,totalPrice:string>

== Whole Stage Codegen ==
Found 4 WholeStageCodegen subtrees.
== Subtree 1 / 4 (maxMethodCodeSize:197; maxConstantPoolSize:106(0.16% used); numInnerClasses:0) ==
*(1) Filter isnotnull(userId#25)
+- FileScan csv [userId#25,totalPrice#27] Batched: false, DataFilters: [isnotnull(userId#25)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/William/IdeaProjects/aurora4spark/aurora4spark-parent/aurora4spa..., PartitionFilters: [], PushedFilters: [IsNotNull(userId)], ReadSchema: struct<userId:string,totalPrice:string>

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ /**
/* 006 */  * Codegened pipeline for stage (id=1)
/* 007 */  * *(1) Filter isnotnull(userId#25)
/* 008 */  * +- FileScan csv [userId#25,totalPrice#27] Batched: false, DataFilters: [isnotnull(userId#25)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/William/IdeaProjects/aurora4spark/aurora4spark-parent/aurora4spa..., PartitionFilters: [], PushedFilters: [IsNotNull(userId)], ReadSchema: struct<userId:string,totalPrice:string>
/* 009 */  */
/* 010 */ // codegenStageId=1
/* 011 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 012 */   private Object[] references;
/* 013 */   private scala.collection.Iterator[] inputs;
/* 014 */   private scala.collection.Iterator inputadapter_input_0;
/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 016 */
/* 017 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 018 */     this.references = references;
/* 019 */   }
/* 020 */
/* 021 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 022 */     partitionIndex = index;
/* 023 */     this.inputs = inputs;
/* 024 */     inputadapter_input_0 = inputs[0];
/* 025 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 64);
/* 026 */
/* 027 */   }
/* 028 */
/* 029 */   protected void processNext() throws java.io.IOException {
/* 030 */     // PRODUCE: Filter isnotnull(userId#25)
/* 031 */     // PRODUCE: InputAdapter
/* 032 */     while ( inputadapter_input_0.hasNext()) {
/* 033 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 034 */
/* 035 */       // CONSUME: Filter isnotnull(userId#25)
/* 036 */       do {
/* 037 */         // input[0, string, true]
/* 038 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 039 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 040 */         null : (inputadapter_row_0.getUTF8String(0));
/* 041 */
/* 042 */         // isnotnull(input[0, string, true])
/* 043 */         boolean filter_value_2 = !inputadapter_isNull_0;
/* 044 */         if (!filter_value_2) continue;
/* 045 */
/* 046 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 047 */
/* 048 */         // CONSUME: WholeStageCodegen (1)
/* 049 */         // input[1, string, true]
/* 050 */         boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 051 */         UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 052 */         null : (inputadapter_row_0.getUTF8String(1));
/* 053 */         filter_mutableStateArray_0[0].reset();
/* 054 */
/* 055 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 058 */
/* 059 */         if (inputadapter_isNull_1) {
/* 060 */           filter_mutableStateArray_0[0].setNullAt(1);
/* 061 */         } else {
/* 062 */           filter_mutableStateArray_0[0].write(1, inputadapter_value_1);
/* 063 */         }
/* 064 */         append((filter_mutableStateArray_0[0].getRow()));
/* 065 */
/* 066 */       } while(false);
/* 067 */       if (shouldStop()) return;
/* 068 */     }
/* 069 */   }
/* 070 */
/* 071 */ }

== Subtree 2 / 4 (maxMethodCodeSize:288; maxConstantPoolSize:319(0.49% used); numInnerClasses:1) ==
*(2) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[partial_sum(cast(totalPrice#27 as double))], output=[userId#25, joined_timestamp#3L, sum#41])
+- *(2) Project [JOINED_TIMESTAMP#3L, userId#25, totalPrice#27]
   +- *(2) BroadcastHashJoin [id#0], [cast(userId#25 as int)], Inner, BuildRight, false
      :- *(2) Scan JDBCRelation(Users) [numPartitions=1] [ID#0,JOINED_TIMESTAMP#3L] PushedFilters: [*IsNotNull(ID)], ReadSchema: struct<ID:int,JOINED_TIMESTAMP:bigint>
      +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, string, false] as int) as bigint)),false), [id=#73]
         +- *(1) Filter isnotnull(userId#25)
            +- FileScan csv [userId#25,totalPrice#27] Batched: false, DataFilters: [isnotnull(userId#25)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/William/IdeaProjects/aurora4spark/aurora4spark-parent/aurora4spa..., PartitionFilters: [], PushedFilters: [IsNotNull(userId)], ReadSchema: struct<userId:string,totalPrice:string>

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage2(references);
/* 003 */ }
/* 004 */
/* 005 */ /**
/* 006 */  * Codegened pipeline for stage (id=2)
/* 007 */  * *(2) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[partial_sum(cast(totalPrice#27 as double))], output=[userId#25, joined_timestamp#3L, sum#41])
/* 008 */  * +- *(2) Project [JOINED_TIMESTAMP#3L, userId#25, totalPrice#27]
/* 009 */  *    +- *(2) BroadcastHashJoin [id#0], [cast(userId#25 as int)], Inner, BuildRight, false
/* 010 */  *       :- *(2) Scan JDBCRelation(Users) [numPartitions=1] [ID#0,JOINED_TIMESTAMP#3L] PushedFilters: [*IsNotNull(ID)], ReadSchema: struct<ID:int,JOINED_TIMESTAMP:bigint>
/* 011 */  *       +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, string, false] as int) as bigint)),false), [id=#73]
/* 012 */  *          +- *(1) Filter isnotnull(userId#25)
/* 013 */  *             +- FileScan csv [userId#25,totalPrice#27] Batched: false, DataFilters: [isnotnull(userId#25)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/William/IdeaProjects/aurora4spark/aurora4spark-parent/aurora4spa..., PartitionFilters: [], PushedFilters: [IsNotNull(userId)], ReadSchema: struct<userId:string,totalPrice:string>
/* 014 */  */
/* 015 */ // codegenStageId=2
/* 016 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 017 */   private Object[] references;
/* 018 */   private scala.collection.Iterator[] inputs;
/* 019 */   private boolean agg_initAgg_0;
/* 020 */   private boolean agg_bufIsNull_0;
/* 021 */   private double agg_bufValue_0;
/* 022 */   private agg_FastHashMap_0 agg_fastHashMap_0;
/* 023 */   private org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> agg_fastHashMapIter_0;
/* 024 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 025 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 026 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 027 */   private scala.collection.Iterator scan_input_0;
/* 028 */   private org.apache.spark.sql.execution.joins.LongHashedRelation bhj_relation_0;
/* 029 */   private boolean agg_agg_isNull_8_0;
/* 030 */   private boolean agg_agg_isNull_10_0;
/* 031 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] scan_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[6];
/* 032 */
/* 033 */   public GeneratedIteratorForCodegenStage2(Object[] references) {
/* 034 */     this.references = references;
/* 035 */   }
/* 036 */
/* 037 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 038 */     partitionIndex = index;
/* 039 */     this.inputs = inputs;
/* 040 */
/* 041 */     scan_input_0 = inputs[0];
/* 042 */     scan_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 0);
/* 043 */
/* 044 */     bhj_relation_0 = ((org.apache.spark.sql.execution.joins.LongHashedRelation) ((org.apache.spark.broadcast.TorrentBroadcast) references[7] /* broadcast */).value()).asReadOnlyCopy();
/* 045 */     incPeakExecutionMemory(bhj_relation_0.estimatedSize());
/* 046 */
/* 047 */     scan_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(4, 64);
/* 048 */     scan_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);
/* 049 */     scan_mutableStateArray_0[3] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);
/* 050 */     scan_mutableStateArray_0[4] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 051 */     scan_mutableStateArray_0[5] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 052 */
/* 053 */   }
/* 054 */
/* 055 */   public class agg_FastHashMap_0 {
/* 056 */     private org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch batch;
/* 057 */     private int[] buckets;
/* 058 */     private int capacity = 1 << 16;
/* 059 */     private double loadFactor = 0.5;
/* 060 */     private int numBuckets = (int) (capacity / loadFactor);
/* 061 */     private int maxSteps = 2;
/* 062 */     private int numRows = 0;
/* 063 */     private Object emptyVBase;
/* 064 */     private long emptyVOff;
/* 065 */     private int emptyVLen;
/* 066 */     private boolean isBatchFull = false;
/* 067 */     private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter agg_rowWriter;
/* 068 */
/* 069 */     public agg_FastHashMap_0(
/* 070 */       org.apache.spark.memory.TaskMemoryManager taskMemoryManager,
/* 071 */       InternalRow emptyAggregationBuffer) {
/* 072 */       batch = org.apache.spark.sql.catalyst.expressions.RowBasedKeyValueBatch
/* 073 */       .allocate(((org.apache.spark.sql.types.StructType) references[1] /* keySchemaTerm */), ((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */), taskMemoryManager, capacity);
/* 074 */
/* 075 */       final UnsafeProjection valueProjection = UnsafeProjection.create(((org.apache.spark.sql.types.StructType) references[2] /* valueSchemaTerm */));
/* 076 */       final byte[] emptyBuffer = valueProjection.apply(emptyAggregationBuffer).getBytes();
/* 077 */
/* 078 */       emptyVBase = emptyBuffer;
/* 079 */       emptyVOff = Platform.BYTE_ARRAY_OFFSET;
/* 080 */       emptyVLen = emptyBuffer.length;
/* 081 */
/* 082 */       agg_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(
/* 083 */         2, 32);
/* 084 */
/* 085 */       buckets = new int[numBuckets];
/* 086 */       java.util.Arrays.fill(buckets, -1);
/* 087 */     }
/* 088 */
/* 089 */     public org.apache.spark.sql.catalyst.expressions.UnsafeRow findOrInsert(UTF8String agg_key_0, long agg_key_1) {
/* 090 */       long h = hash(agg_key_0, agg_key_1);
/* 091 */       int step = 0;
/* 092 */       int idx = (int) h & (numBuckets - 1);
/* 093 */       while (step < maxSteps) {
/* 094 */         // Return bucket index if it's either an empty slot or already contains the key
/* 095 */         if (buckets[idx] == -1) {
/* 096 */           if (numRows < capacity && !isBatchFull) {
/* 097 */             agg_rowWriter.reset();
/* 098 */             agg_rowWriter.zeroOutNullBytes();
/* 099 */             agg_rowWriter.write(0, agg_key_0);
/* 100 */             agg_rowWriter.write(1, agg_key_1);
/* 101 */             org.apache.spark.sql.catalyst.expressions.UnsafeRow agg_result
/* 102 */             = agg_rowWriter.getRow();
/* 103 */             Object kbase = agg_result.getBaseObject();
/* 104 */             long koff = agg_result.getBaseOffset();
/* 105 */             int klen = agg_result.getSizeInBytes();
/* 106 */
/* 107 */             UnsafeRow vRow
/* 108 */             = batch.appendRow(kbase, koff, klen, emptyVBase, emptyVOff, emptyVLen);
/* 109 */             if (vRow == null) {
/* 110 */               isBatchFull = true;
/* 111 */             } else {
/* 112 */               buckets[idx] = numRows++;
/* 113 */             }
/* 114 */             return vRow;
/* 115 */           } else {
/* 116 */             // No more space
/* 117 */             return null;
/* 118 */           }
/* 119 */         } else if (equals(idx, agg_key_0, agg_key_1)) {
/* 120 */           return batch.getValueRow(buckets[idx]);
/* 121 */         }
/* 122 */         idx = (idx + 1) & (numBuckets - 1);
/* 123 */         step++;
/* 124 */       }
/* 125 */       // Didn't find it
/* 126 */       return null;
/* 127 */     }
/* 128 */
/* 129 */     private boolean equals(int idx, UTF8String agg_key_0, long agg_key_1) {
/* 130 */       UnsafeRow row = batch.getKeyRow(buckets[idx]);
/* 131 */       return (row.getUTF8String(0).equals(agg_key_0)) && (row.getLong(1) == agg_key_1);
/* 132 */     }
/* 133 */
/* 134 */     private long hash(UTF8String agg_key_0, long agg_key_1) {
/* 135 */       long agg_hash_0 = 0;
/* 136 */
/* 137 */       int agg_result_0 = 0;
/* 138 */       byte[] agg_bytes_0 = agg_key_0.getBytes();
/* 139 */       for (int i = 0; i < agg_bytes_0.length; i++) {
/* 140 */         int agg_hash_1 = agg_bytes_0[i];
/* 141 */         agg_result_0 = (agg_result_0 ^ (0x9e3779b9)) + agg_hash_1 + (agg_result_0 << 6) + (agg_result_0 >>> 2);
/* 142 */       }
/* 143 */
/* 144 */       agg_hash_0 = (agg_hash_0 ^ (0x9e3779b9)) + agg_result_0 + (agg_hash_0 << 6) + (agg_hash_0 >>> 2);
/* 145 */
/* 146 */       long agg_result_1 = agg_key_1;
/* 147 */       agg_hash_0 = (agg_hash_0 ^ (0x9e3779b9)) + agg_result_1 + (agg_hash_0 << 6) + (agg_hash_0 >>> 2);
/* 148 */
/* 149 */       return agg_hash_0;
/* 150 */     }
/* 151 */
/* 152 */     public org.apache.spark.unsafe.KVIterator<UnsafeRow, UnsafeRow> rowIterator() {
/* 153 */       return batch.rowIterator();
/* 154 */     }
/* 155 */
/* 156 */     public void close() {
/* 157 */       batch.close();
/* 158 */     }
/* 159 */
/* 160 */   }
/* 161 */
/* 162 */   private void agg_doAggregate_sum_0(org.apache.spark.unsafe.types.UTF8String agg_expr_2_0, org.apache.spark.sql.catalyst.InternalRow agg_unsafeRowAggBuffer_0, boolean agg_exprIsNull_2_0) throws java.io.IOException {
/* 163 */     // evaluate aggregate function for sum
/* 164 */     // coalesce((coalesce(input[0, double, true], 0.0) + cast(input[3, string, true] as double)), input[0, double, true])
/* 165 */     agg_agg_isNull_8_0 = true;
/* 166 */     double agg_value_9 = -1.0;
/* 167 */     do {
/* 168 */       // (coalesce(input[0, double, true], 0.0) + cast(input[3, string, true] as double))
/* 169 */       boolean agg_isNull_9 = true;
/* 170 */       double agg_value_10 = -1.0;
/* 171 */       // coalesce(input[0, double, true], 0.0)
/* 172 */       agg_agg_isNull_10_0 = true;
/* 173 */       double agg_value_11 = -1.0;
/* 174 */       do {
/* 175 */         // input[0, double, true]
/* 176 */         boolean agg_isNull_11 = agg_unsafeRowAggBuffer_0.isNullAt(0);
/* 177 */         double agg_value_12 = agg_isNull_11 ?
/* 178 */         -1.0 : (agg_unsafeRowAggBuffer_0.getDouble(0));
/* 179 */         if (!agg_isNull_11) {
/* 180 */           agg_agg_isNull_10_0 = false;
/* 181 */           agg_value_11 = agg_value_12;
/* 182 */           continue;
/* 183 */         }
/* 184 */
/* 185 */         if (!false) {
/* 186 */           agg_agg_isNull_10_0 = false;
/* 187 */           agg_value_11 = 0.0D;
/* 188 */           continue;
/* 189 */         }
/* 190 */
/* 191 */       } while (false);
/* 192 */       // cast(input[3, string, true] as double)
/* 193 */       boolean agg_isNull_13 = agg_exprIsNull_2_0;
/* 194 */       double agg_value_14 = -1.0;
/* 195 */       if (!agg_exprIsNull_2_0) {
/* 196 */         final String agg_doubleStr_0 = agg_expr_2_0.toString();
/* 197 */         try {
/* 198 */           agg_value_14 = Double.valueOf(agg_doubleStr_0);
/* 199 */         } catch (java.lang.NumberFormatException e) {
/* 200 */           final Double d = (Double) Cast.processFloatingPointSpecialLiterals(agg_doubleStr_0, false);
/* 201 */           if (d == null) {
/* 202 */             agg_isNull_13 = true;
/* 203 */           } else {
/* 204 */             agg_value_14 = d.doubleValue();
/* 205 */           }
/* 206 */         }
/* 207 */       }
/* 208 */       if (!agg_isNull_13) {
/* 209 */         agg_isNull_9 = false; // resultCode could change nullability.
/* 210 */
/* 211 */         agg_value_10 = agg_value_11 + agg_value_14;
/* 212 */
/* 213 */       }
/* 214 */       if (!agg_isNull_9) {
/* 215 */         agg_agg_isNull_8_0 = false;
/* 216 */         agg_value_9 = agg_value_10;
/* 217 */         continue;
/* 218 */       }
/* 219 */
/* 220 */       // input[0, double, true]
/* 221 */       boolean agg_isNull_15 = agg_unsafeRowAggBuffer_0.isNullAt(0);
/* 222 */       double agg_value_16 = agg_isNull_15 ?
/* 223 */       -1.0 : (agg_unsafeRowAggBuffer_0.getDouble(0));
/* 224 */       if (!agg_isNull_15) {
/* 225 */         agg_agg_isNull_8_0 = false;
/* 226 */         agg_value_9 = agg_value_16;
/* 227 */         continue;
/* 228 */       }
/* 229 */
/* 230 */     } while (false);
/* 231 */     // update unsafe row buffer
/* 232 */     if (!agg_agg_isNull_8_0) {
/* 233 */       agg_unsafeRowAggBuffer_0.setDouble(0, agg_value_9);
/* 234 */     } else {
/* 235 */       agg_unsafeRowAggBuffer_0.setNullAt(0);
/* 236 */     }
/* 237 */   }
/* 238 */
/* 239 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 240 */   throws java.io.IOException {
/* 241 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[9] /* numOutputRows */).add(1);
/* 242 */
/* 243 */     // input[0, string, true]
/* 244 */     boolean agg_isNull_16 = agg_keyTerm_0.isNullAt(0);
/* 245 */     UTF8String agg_value_17 = agg_isNull_16 ?
/* 246 */     null : (agg_keyTerm_0.getUTF8String(0));
/* 247 */     // input[1, bigint, true]
/* 248 */     boolean agg_isNull_17 = agg_keyTerm_0.isNullAt(1);
/* 249 */     long agg_value_18 = agg_isNull_17 ?
/* 250 */     -1L : (agg_keyTerm_0.getLong(1));
/* 251 */     // input[0, double, true]
/* 252 */     boolean agg_isNull_18 = agg_bufferTerm_0.isNullAt(0);
/* 253 */     double agg_value_19 = agg_isNull_18 ?
/* 254 */     -1.0 : (agg_bufferTerm_0.getDouble(0));
/* 255 */
/* 256 */     // CONSUME: WholeStageCodegen (2)
/* 257 */     scan_mutableStateArray_0[5].reset();
/* 258 */
/* 259 */     scan_mutableStateArray_0[5].zeroOutNullBytes();
/* 260 */
/* 261 */     if (agg_isNull_16) {
/* 262 */       scan_mutableStateArray_0[5].setNullAt(0);
/* 263 */     } else {
/* 264 */       scan_mutableStateArray_0[5].write(0, agg_value_17);
/* 265 */     }
/* 266 */
/* 267 */     if (agg_isNull_17) {
/* 268 */       scan_mutableStateArray_0[5].setNullAt(1);
/* 269 */     } else {
/* 270 */       scan_mutableStateArray_0[5].write(1, agg_value_18);
/* 271 */     }
/* 272 */
/* 273 */     if (agg_isNull_18) {
/* 274 */       scan_mutableStateArray_0[5].setNullAt(2);
/* 275 */     } else {
/* 276 */       scan_mutableStateArray_0[5].write(2, agg_value_19);
/* 277 */     }
/* 278 */     append((scan_mutableStateArray_0[5].getRow()));
/* 279 */
/* 280 */   }
/* 281 */
/* 282 */   private void agg_doConsume_0(long agg_expr_0_0, boolean agg_exprIsNull_0_0, UTF8String agg_expr_1_0, boolean agg_exprIsNull_1_0, UTF8String agg_expr_2_0, boolean agg_exprIsNull_2_0) throws java.io.IOException {
/* 283 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 284 */     UnsafeRow agg_fastAggBuffer_0 = null;
/* 285 */
/* 286 */     if (true) {
/* 287 */       if (!agg_exprIsNull_1_0 && !agg_exprIsNull_0_0) {
/* 288 */         agg_fastAggBuffer_0 = agg_fastHashMap_0.findOrInsert(
/* 289 */           agg_expr_1_0, agg_expr_0_0);
/* 290 */       }
/* 291 */     }
/* 292 */     // Cannot find the key in fast hash map, try regular hash map.
/* 293 */     if (agg_fastAggBuffer_0 == null) {
/* 294 */       // generate grouping key
/* 295 */       scan_mutableStateArray_0[4].reset();
/* 296 */
/* 297 */       scan_mutableStateArray_0[4].zeroOutNullBytes();
/* 298 */
/* 299 */       if (agg_exprIsNull_1_0) {
/* 300 */         scan_mutableStateArray_0[4].setNullAt(0);
/* 301 */       } else {
/* 302 */         scan_mutableStateArray_0[4].write(0, agg_expr_1_0);
/* 303 */       }
/* 304 */
/* 305 */       if (agg_exprIsNull_0_0) {
/* 306 */         scan_mutableStateArray_0[4].setNullAt(1);
/* 307 */       } else {
/* 308 */         scan_mutableStateArray_0[4].write(1, agg_expr_0_0);
/* 309 */       }
/* 310 */       int agg_unsafeRowKeyHash_0 = (scan_mutableStateArray_0[4].getRow()).hashCode();
/* 311 */       if (true) {
/* 312 */         // try to get the buffer from hash map
/* 313 */         agg_unsafeRowAggBuffer_0 =
/* 314 */         agg_hashMap_0.getAggregationBufferFromUnsafeRow((scan_mutableStateArray_0[4].getRow()), agg_unsafeRowKeyHash_0);
/* 315 */       }
/* 316 */       // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 317 */       // aggregation after processing all input rows.
/* 318 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 319 */         if (agg_sorter_0 == null) {
/* 320 */           agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 321 */         } else {
/* 322 */           agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 323 */         }
/* 324 */
/* 325 */         // the hash map had be spilled, it should have enough memory now,
/* 326 */         // try to allocate buffer again.
/* 327 */         agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 328 */           (scan_mutableStateArray_0[4].getRow()), agg_unsafeRowKeyHash_0);
/* 329 */         if (agg_unsafeRowAggBuffer_0 == null) {
/* 330 */           // failed to allocate the first page
/* 331 */           throw new org.apache.spark.memory.SparkOutOfMemoryError("No enough memory for aggregation");
/* 332 */         }
/* 333 */       }
/* 334 */
/* 335 */     }
/* 336 */
/* 337 */     // Updates the proper row buffer
/* 338 */     if (agg_fastAggBuffer_0 != null) {
/* 339 */       agg_unsafeRowAggBuffer_0 = agg_fastAggBuffer_0;
/* 340 */     }
/* 341 */
/* 342 */     // common sub-expressions
/* 343 */
/* 344 */     // evaluate aggregate functions and update aggregation buffers
/* 345 */     agg_doAggregate_sum_0(agg_expr_2_0, agg_unsafeRowAggBuffer_0, agg_exprIsNull_2_0);
/* 346 */
/* 347 */   }
/* 348 */
/* 349 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 350 */     // PRODUCE: Project [JOINED_TIMESTAMP#3L, userId#25, totalPrice#27]
/* 351 */     // PRODUCE: BroadcastHashJoin [id#0], [cast(userId#25 as int)], Inner, BuildRight, false
/* 352 */     // PRODUCE: Scan JDBCRelation(Users) [numPartitions=1] [ID#0,JOINED_TIMESTAMP#3L] PushedFilters: [*IsNotNull(ID)], ReadSchema: struct<ID:int,JOINED_TIMESTAMP:bigint>
/* 353 */     while ( scan_input_0.hasNext()) {
/* 354 */       InternalRow scan_row_0 = (InternalRow) scan_input_0.next();
/* 355 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[6] /* numOutputRows */).add(1);
/* 356 */       // CONSUME: BroadcastHashJoin [id#0], [cast(userId#25 as int)], Inner, BuildRight, false
/* 357 */       // input[0, int, true]
/* 358 */       boolean scan_isNull_0 = scan_row_0.isNullAt(0);
/* 359 */       int scan_value_0 = scan_isNull_0 ?
/* 360 */       -1 : (scan_row_0.getInt(0));
/* 361 */
/* 362 */       // generate join key for stream side
/* 363 */       // cast(input[0, int, true] as bigint)
/* 364 */       boolean bhj_isNull_0 = scan_isNull_0;
/* 365 */       long bhj_value_0 = -1L;
/* 366 */       if (!scan_isNull_0) {
/* 367 */         bhj_value_0 = (long) scan_value_0;
/* 368 */       }
/* 369 */       // find matches from HashRelation
/* 370 */       scala.collection.Iterator bhj_matches_0 = bhj_isNull_0 ?
/* 371 */       null : (scala.collection.Iterator)bhj_relation_0.get(bhj_value_0);
/* 372 */       if (bhj_matches_0 != null) {
/* 373 */         while (bhj_matches_0.hasNext()) {
/* 374 */           UnsafeRow bhj_matched_0 = (UnsafeRow) bhj_matches_0.next();
/* 375 */           {
/* 376 */             ((org.apache.spark.sql.execution.metric.SQLMetric) references[8] /* numOutputRows */).add(1);
/* 377 */
/* 378 */             // CONSUME: Project [JOINED_TIMESTAMP#3L, userId#25, totalPrice#27]
/* 379 */             // common sub-expressions
/* 380 */
/* 381 */             // CONSUME: HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[partial_sum(cast(totalPrice#27 as double))])
/* 382 */             // input[1, bigint, true]
/* 383 */             boolean scan_isNull_1 = scan_row_0.isNullAt(1);
/* 384 */             long scan_value_1 = scan_isNull_1 ?
/* 385 */             -1L : (scan_row_0.getLong(1));
/* 386 */             // input[2, string, false]
/* 387 */             // input[0, string, false]
/* 388 */             UTF8String bhj_value_2 = bhj_matched_0.getUTF8String(0);
/* 389 */             // input[3, string, true]
/* 390 */             // input[1, string, true]
/* 391 */             boolean bhj_isNull_3 = bhj_matched_0.isNullAt(1);
/* 392 */             UTF8String bhj_value_3 = bhj_isNull_3 ?
/* 393 */             null : (bhj_matched_0.getUTF8String(1));
/* 394 */
/* 395 */             agg_doConsume_0(scan_value_1, scan_isNull_1, bhj_value_2, false, bhj_value_3, bhj_isNull_3);
/* 396 */
/* 397 */           }
/* 398 */         }
/* 399 */       }
/* 400 */       // shouldStop check is eliminated
/* 401 */     }
/* 402 */
/* 403 */     agg_fastHashMapIter_0 = agg_fastHashMap_0.rowIterator();
/* 404 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* avgHashProbe */));
/* 405 */
/* 406 */   }
/* 407 */
/* 408 */   protected void processNext() throws java.io.IOException {
/* 409 */     // PRODUCE: HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[partial_sum(cast(totalPrice#27 as double))])
/* 410 */     if (!agg_initAgg_0) {
/* 411 */       agg_initAgg_0 = true;
/* 412 */       agg_fastHashMap_0 = new agg_FastHashMap_0(((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getTaskMemoryManager(), ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).getEmptyAggregationBuffer());
/* 413 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 414 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 415 */       agg_doAggregateWithKeys_0();
/* 416 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[10] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 417 */     }
/* 418 */     // output the result
/* 419 */
/* 420 */     while (agg_fastHashMapIter_0.next()) {
/* 421 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_fastHashMapIter_0.getKey();
/* 422 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_fastHashMapIter_0.getValue();
/* 423 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 424 */
/* 425 */       if (shouldStop()) return;
/* 426 */     }
/* 427 */     agg_fastHashMap_0.close();
/* 428 */
/* 429 */     while ( agg_mapIter_0.next()) {
/* 430 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 431 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 432 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 433 */       if (shouldStop()) return;
/* 434 */     }
/* 435 */     agg_mapIter_0.close();
/* 436 */     if (agg_sorter_0 == null) {
/* 437 */       agg_hashMap_0.free();
/* 438 */     }
/* 439 */   }
/* 440 */
/* 441 */ }

== Subtree 3 / 4 (maxMethodCodeSize:206; maxConstantPoolSize:232(0.35% used); numInnerClasses:0) ==
*(3) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[sum(cast(totalPrice#27 as double))], output=[sum(CAST(totalPrice AS DOUBLE))#35, userId#25, joined_timestamp#3L])
+- Exchange hashpartitioning(userId#25, joined_timestamp#3L, 200), ENSURE_REQUIREMENTS, [id=#79]
   +- *(2) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[partial_sum(cast(totalPrice#27 as double))], output=[userId#25, joined_timestamp#3L, sum#41])
      +- *(2) Project [JOINED_TIMESTAMP#3L, userId#25, totalPrice#27]
         +- *(2) BroadcastHashJoin [id#0], [cast(userId#25 as int)], Inner, BuildRight, false
            :- *(2) Scan JDBCRelation(Users) [numPartitions=1] [ID#0,JOINED_TIMESTAMP#3L] PushedFilters: [*IsNotNull(ID)], ReadSchema: struct<ID:int,JOINED_TIMESTAMP:bigint>
            +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, string, false] as int) as bigint)),false), [id=#73]
               +- *(1) Filter isnotnull(userId#25)
                  +- FileScan csv [userId#25,totalPrice#27] Batched: false, DataFilters: [isnotnull(userId#25)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/William/IdeaProjects/aurora4spark/aurora4spark-parent/aurora4spa..., PartitionFilters: [], PushedFilters: [IsNotNull(userId)], ReadSchema: struct<userId:string,totalPrice:string>

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage3(references);
/* 003 */ }
/* 004 */
/* 005 */ /**
/* 006 */  * Codegened pipeline for stage (id=3)
/* 007 */  * *(3) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[sum(cast(totalPrice#27 as double))], output=[sum(CAST(totalPrice AS DOUBLE))#35, userId#25, joined_timestamp#3L])
/* 008 */  * +- Exchange hashpartitioning(userId#25, joined_timestamp#3L, 200), ENSURE_REQUIREMENTS, [id=#79]
/* 009 */  *    +- *(2) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[partial_sum(cast(totalPrice#27 as double))], output=[userId#25, joined_timestamp#3L, sum#41])
/* 010 */  *       +- *(2) Project [JOINED_TIMESTAMP#3L, userId#25, totalPrice#27]
/* 011 */  *          +- *(2) BroadcastHashJoin [id#0], [cast(userId#25 as int)], Inner, BuildRight, false
/* 012 */  *             :- *(2) Scan JDBCRelation(Users) [numPartitions=1] [ID#0,JOINED_TIMESTAMP#3L] PushedFilters: [*IsNotNull(ID)], ReadSchema: struct<ID:int,JOINED_TIMESTAMP:bigint>
/* 013 */  *             +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, string, false] as int) as bigint)),false), [id=#73]
/* 014 */  *                +- *(1) Filter isnotnull(userId#25)
/* 015 */  *                   +- FileScan csv [userId#25,totalPrice#27] Batched: false, DataFilters: [isnotnull(userId#25)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/William/IdeaProjects/aurora4spark/aurora4spark-parent/aurora4spa..., PartitionFilters: [], PushedFilters: [IsNotNull(userId)], ReadSchema: struct<userId:string,totalPrice:string>
/* 016 */  */
/* 017 */ // codegenStageId=3
/* 018 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 019 */   private Object[] references;
/* 020 */   private scala.collection.Iterator[] inputs;
/* 021 */   private boolean agg_initAgg_0;
/* 022 */   private org.apache.spark.unsafe.KVIterator agg_mapIter_0;
/* 023 */   private org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap agg_hashMap_0;
/* 024 */   private org.apache.spark.sql.execution.UnsafeKVExternalSorter agg_sorter_0;
/* 025 */   private scala.collection.Iterator inputadapter_input_0;
/* 026 */   private boolean agg_agg_isNull_4_0;
/* 027 */   private boolean agg_agg_isNull_6_0;
/* 028 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] agg_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[2];
/* 029 */
/* 030 */   public GeneratedIteratorForCodegenStage3(Object[] references) {
/* 031 */     this.references = references;
/* 032 */   }
/* 033 */
/* 034 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 035 */     partitionIndex = index;
/* 036 */     this.inputs = inputs;
/* 037 */
/* 038 */     inputadapter_input_0 = inputs[0];
/* 039 */     agg_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(2, 32);
/* 040 */     agg_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);
/* 041 */
/* 042 */   }
/* 043 */
/* 044 */   private void agg_doAggregate_sum_0(org.apache.spark.sql.catalyst.InternalRow agg_unsafeRowAggBuffer_0, double agg_expr_2_0, boolean agg_exprIsNull_2_0) throws java.io.IOException {
/* 045 */     // evaluate aggregate function for sum
/* 046 */     // coalesce((coalesce(input[0, double, true], 0.0) + input[3, double, true]), input[0, double, true])
/* 047 */     agg_agg_isNull_4_0 = true;
/* 048 */     double agg_value_4 = -1.0;
/* 049 */     do {
/* 050 */       // (coalesce(input[0, double, true], 0.0) + input[3, double, true])
/* 051 */       boolean agg_isNull_5 = true;
/* 052 */       double agg_value_5 = -1.0;
/* 053 */       // coalesce(input[0, double, true], 0.0)
/* 054 */       agg_agg_isNull_6_0 = true;
/* 055 */       double agg_value_6 = -1.0;
/* 056 */       do {
/* 057 */         // input[0, double, true]
/* 058 */         boolean agg_isNull_7 = agg_unsafeRowAggBuffer_0.isNullAt(0);
/* 059 */         double agg_value_7 = agg_isNull_7 ?
/* 060 */         -1.0 : (agg_unsafeRowAggBuffer_0.getDouble(0));
/* 061 */         if (!agg_isNull_7) {
/* 062 */           agg_agg_isNull_6_0 = false;
/* 063 */           agg_value_6 = agg_value_7;
/* 064 */           continue;
/* 065 */         }
/* 066 */
/* 067 */         if (!false) {
/* 068 */           agg_agg_isNull_6_0 = false;
/* 069 */           agg_value_6 = 0.0D;
/* 070 */           continue;
/* 071 */         }
/* 072 */
/* 073 */       } while (false);
/* 074 */
/* 075 */       if (!agg_exprIsNull_2_0) {
/* 076 */         agg_isNull_5 = false; // resultCode could change nullability.
/* 077 */
/* 078 */         agg_value_5 = agg_value_6 + agg_expr_2_0;
/* 079 */
/* 080 */       }
/* 081 */       if (!agg_isNull_5) {
/* 082 */         agg_agg_isNull_4_0 = false;
/* 083 */         agg_value_4 = agg_value_5;
/* 084 */         continue;
/* 085 */       }
/* 086 */
/* 087 */       // input[0, double, true]
/* 088 */       boolean agg_isNull_10 = agg_unsafeRowAggBuffer_0.isNullAt(0);
/* 089 */       double agg_value_10 = agg_isNull_10 ?
/* 090 */       -1.0 : (agg_unsafeRowAggBuffer_0.getDouble(0));
/* 091 */       if (!agg_isNull_10) {
/* 092 */         agg_agg_isNull_4_0 = false;
/* 093 */         agg_value_4 = agg_value_10;
/* 094 */         continue;
/* 095 */       }
/* 096 */
/* 097 */     } while (false);
/* 098 */     // update unsafe row buffer
/* 099 */     if (!agg_agg_isNull_4_0) {
/* 100 */       agg_unsafeRowAggBuffer_0.setDouble(0, agg_value_4);
/* 101 */     } else {
/* 102 */       agg_unsafeRowAggBuffer_0.setNullAt(0);
/* 103 */     }
/* 104 */   }
/* 105 */
/* 106 */   private void agg_doAggregateWithKeysOutput_0(UnsafeRow agg_keyTerm_0, UnsafeRow agg_bufferTerm_0)
/* 107 */   throws java.io.IOException {
/* 108 */     ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* numOutputRows */).add(1);
/* 109 */
/* 110 */     // input[0, string, true]
/* 111 */     boolean agg_isNull_11 = agg_keyTerm_0.isNullAt(0);
/* 112 */     UTF8String agg_value_11 = agg_isNull_11 ?
/* 113 */     null : (agg_keyTerm_0.getUTF8String(0));
/* 114 */     // input[1, bigint, true]
/* 115 */     boolean agg_isNull_12 = agg_keyTerm_0.isNullAt(1);
/* 116 */     long agg_value_12 = agg_isNull_12 ?
/* 117 */     -1L : (agg_keyTerm_0.getLong(1));
/* 118 */     // input[0, double, true]
/* 119 */     boolean agg_isNull_13 = agg_bufferTerm_0.isNullAt(0);
/* 120 */     double agg_value_13 = agg_isNull_13 ?
/* 121 */     -1.0 : (agg_bufferTerm_0.getDouble(0));
/* 122 */
/* 123 */     // CONSUME: WholeStageCodegen (3)
/* 124 */     agg_mutableStateArray_0[1].reset();
/* 125 */
/* 126 */     agg_mutableStateArray_0[1].zeroOutNullBytes();
/* 127 */
/* 128 */     if (agg_isNull_13) {
/* 129 */       agg_mutableStateArray_0[1].setNullAt(0);
/* 130 */     } else {
/* 131 */       agg_mutableStateArray_0[1].write(0, agg_value_13);
/* 132 */     }
/* 133 */
/* 134 */     if (agg_isNull_11) {
/* 135 */       agg_mutableStateArray_0[1].setNullAt(1);
/* 136 */     } else {
/* 137 */       agg_mutableStateArray_0[1].write(1, agg_value_11);
/* 138 */     }
/* 139 */
/* 140 */     if (agg_isNull_12) {
/* 141 */       agg_mutableStateArray_0[1].setNullAt(2);
/* 142 */     } else {
/* 143 */       agg_mutableStateArray_0[1].write(2, agg_value_12);
/* 144 */     }
/* 145 */     append((agg_mutableStateArray_0[1].getRow()));
/* 146 */
/* 147 */   }
/* 148 */
/* 149 */   private void agg_doConsume_0(InternalRow inputadapter_row_0, UTF8String agg_expr_0_0, boolean agg_exprIsNull_0_0, long agg_expr_1_0, boolean agg_exprIsNull_1_0, double agg_expr_2_0, boolean agg_exprIsNull_2_0) throws java.io.IOException {
/* 150 */     UnsafeRow agg_unsafeRowAggBuffer_0 = null;
/* 151 */
/* 152 */     // generate grouping key
/* 153 */     agg_mutableStateArray_0[0].reset();
/* 154 */
/* 155 */     agg_mutableStateArray_0[0].zeroOutNullBytes();
/* 156 */
/* 157 */     if (agg_exprIsNull_0_0) {
/* 158 */       agg_mutableStateArray_0[0].setNullAt(0);
/* 159 */     } else {
/* 160 */       agg_mutableStateArray_0[0].write(0, agg_expr_0_0);
/* 161 */     }
/* 162 */
/* 163 */     if (agg_exprIsNull_1_0) {
/* 164 */       agg_mutableStateArray_0[0].setNullAt(1);
/* 165 */     } else {
/* 166 */       agg_mutableStateArray_0[0].write(1, agg_expr_1_0);
/* 167 */     }
/* 168 */     int agg_unsafeRowKeyHash_0 = (agg_mutableStateArray_0[0].getRow()).hashCode();
/* 169 */     if (true) {
/* 170 */       // try to get the buffer from hash map
/* 171 */       agg_unsafeRowAggBuffer_0 =
/* 172 */       agg_hashMap_0.getAggregationBufferFromUnsafeRow((agg_mutableStateArray_0[0].getRow()), agg_unsafeRowKeyHash_0);
/* 173 */     }
/* 174 */     // Can't allocate buffer from the hash map. Spill the map and fallback to sort-based
/* 175 */     // aggregation after processing all input rows.
/* 176 */     if (agg_unsafeRowAggBuffer_0 == null) {
/* 177 */       if (agg_sorter_0 == null) {
/* 178 */         agg_sorter_0 = agg_hashMap_0.destructAndCreateExternalSorter();
/* 179 */       } else {
/* 180 */         agg_sorter_0.merge(agg_hashMap_0.destructAndCreateExternalSorter());
/* 181 */       }
/* 182 */
/* 183 */       // the hash map had be spilled, it should have enough memory now,
/* 184 */       // try to allocate buffer again.
/* 185 */       agg_unsafeRowAggBuffer_0 = agg_hashMap_0.getAggregationBufferFromUnsafeRow(
/* 186 */         (agg_mutableStateArray_0[0].getRow()), agg_unsafeRowKeyHash_0);
/* 187 */       if (agg_unsafeRowAggBuffer_0 == null) {
/* 188 */         // failed to allocate the first page
/* 189 */         throw new org.apache.spark.memory.SparkOutOfMemoryError("No enough memory for aggregation");
/* 190 */       }
/* 191 */     }
/* 192 */
/* 193 */     // common sub-expressions
/* 194 */
/* 195 */     // evaluate aggregate functions and update aggregation buffers
/* 196 */     agg_doAggregate_sum_0(agg_unsafeRowAggBuffer_0, agg_expr_2_0, agg_exprIsNull_2_0);
/* 197 */
/* 198 */   }
/* 199 */
/* 200 */   private void agg_doAggregateWithKeys_0() throws java.io.IOException {
/* 201 */     // PRODUCE: InputAdapter
/* 202 */     while ( inputadapter_input_0.hasNext()) {
/* 203 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 204 */
/* 205 */       // CONSUME: HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[sum(cast(totalPrice#27 as double))])
/* 206 */       // input[0, string, true]
/* 207 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 208 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 209 */       null : (inputadapter_row_0.getUTF8String(0));
/* 210 */       // input[1, bigint, true]
/* 211 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 212 */       long inputadapter_value_1 = inputadapter_isNull_1 ?
/* 213 */       -1L : (inputadapter_row_0.getLong(1));
/* 214 */       // input[2, double, true]
/* 215 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 216 */       double inputadapter_value_2 = inputadapter_isNull_2 ?
/* 217 */       -1.0 : (inputadapter_row_0.getDouble(2));
/* 218 */
/* 219 */       agg_doConsume_0(inputadapter_row_0, inputadapter_value_0, inputadapter_isNull_0, inputadapter_value_1, inputadapter_isNull_1, inputadapter_value_2, inputadapter_isNull_2);
/* 220 */       // shouldStop check is eliminated
/* 221 */     }
/* 222 */
/* 223 */     agg_mapIter_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).finishAggregate(agg_hashMap_0, agg_sorter_0, ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */), ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* avgHashProbe */));
/* 224 */   }
/* 225 */
/* 226 */   protected void processNext() throws java.io.IOException {
/* 227 */     // PRODUCE: HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[sum(cast(totalPrice#27 as double))])
/* 228 */     if (!agg_initAgg_0) {
/* 229 */       agg_initAgg_0 = true;
/* 230 */
/* 231 */       agg_hashMap_0 = ((org.apache.spark.sql.execution.aggregate.HashAggregateExec) references[0] /* plan */).createHashMap();
/* 232 */       long wholestagecodegen_beforeAgg_0 = System.nanoTime();
/* 233 */       agg_doAggregateWithKeys_0();
/* 234 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[5] /* aggTime */).add((System.nanoTime() - wholestagecodegen_beforeAgg_0) / 1000000);
/* 235 */     }
/* 236 */     // output the result
/* 237 */
/* 238 */     while ( agg_mapIter_0.next()) {
/* 239 */       UnsafeRow agg_aggKey_0 = (UnsafeRow) agg_mapIter_0.getKey();
/* 240 */       UnsafeRow agg_aggBuffer_0 = (UnsafeRow) agg_mapIter_0.getValue();
/* 241 */       agg_doAggregateWithKeysOutput_0(agg_aggKey_0, agg_aggBuffer_0);
/* 242 */       if (shouldStop()) return;
/* 243 */     }
/* 244 */     agg_mapIter_0.close();
/* 245 */     if (agg_sorter_0 == null) {
/* 246 */       agg_hashMap_0.free();
/* 247 */     }
/* 248 */   }
/* 249 */
/* 250 */ }

== Subtree 4 / 4 (maxMethodCodeSize:154; maxConstantPoolSize:130(0.20% used); numInnerClasses:0) ==
*(4) Sort [joined_timestamp#3L ASC NULLS FIRST], true, 0
+- Exchange rangepartitioning(joined_timestamp#3L ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#83]
   +- *(3) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[sum(cast(totalPrice#27 as double))], output=[sum(CAST(totalPrice AS DOUBLE))#35, userId#25, joined_timestamp#3L])
      +- Exchange hashpartitioning(userId#25, joined_timestamp#3L, 200), ENSURE_REQUIREMENTS, [id=#79]
         +- *(2) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[partial_sum(cast(totalPrice#27 as double))], output=[userId#25, joined_timestamp#3L, sum#41])
            +- *(2) Project [JOINED_TIMESTAMP#3L, userId#25, totalPrice#27]
               +- *(2) BroadcastHashJoin [id#0], [cast(userId#25 as int)], Inner, BuildRight, false
                  :- *(2) Scan JDBCRelation(Users) [numPartitions=1] [ID#0,JOINED_TIMESTAMP#3L] PushedFilters: [*IsNotNull(ID)], ReadSchema: struct<ID:int,JOINED_TIMESTAMP:bigint>
                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, string, false] as int) as bigint)),false), [id=#73]
                     +- *(1) Filter isnotnull(userId#25)
                        +- FileScan csv [userId#25,totalPrice#27] Batched: false, DataFilters: [isnotnull(userId#25)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/William/IdeaProjects/aurora4spark/aurora4spark-parent/aurora4spa..., PartitionFilters: [], PushedFilters: [IsNotNull(userId)], ReadSchema: struct<userId:string,totalPrice:string>

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage4(references);
/* 003 */ }
/* 004 */
/* 005 */ /**
/* 006 */  * Codegened pipeline for stage (id=4)
/* 007 */  * *(4) Sort [joined_timestamp#3L ASC NULLS FIRST], true, 0
/* 008 */  * +- Exchange rangepartitioning(joined_timestamp#3L ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#83]
/* 009 */  *    +- *(3) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[sum(cast(totalPrice#27 as double))], output=[sum(CAST(totalPrice AS DOUBLE))#35, userId#25, joined_timestamp#3L])
/* 010 */  *       +- Exchange hashpartitioning(userId#25, joined_timestamp#3L, 200), ENSURE_REQUIREMENTS, [id=#79]
/* 011 */  *          +- *(2) HashAggregate(keys=[userId#25, joined_timestamp#3L], functions=[partial_sum(cast(totalPrice#27 as double))], output=[userId#25, joined_timestamp#3L, sum#41])
/* 012 */  *             +- *(2) Project [JOINED_TIMESTAMP#3L, userId#25, totalPrice#27]
/* 013 */  *                +- *(2) BroadcastHashJoin [id#0], [cast(userId#25 as int)], Inner, BuildRight, false
/* 014 */  *                   :- *(2) Scan JDBCRelation(Users) [numPartitions=1] [ID#0,JOINED_TIMESTAMP#3L] PushedFilters: [*IsNotNull(ID)], ReadSchema: struct<ID:int,JOINED_TIMESTAMP:bigint>
/* 015 */  *                   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(cast(input[0, string, false] as int) as bigint)),false), [id=#73]
/* 016 */  *                      +- *(1) Filter isnotnull(userId#25)
/* 017 */  *                         +- FileScan csv [userId#25,totalPrice#27] Batched: false, DataFilters: [isnotnull(userId#25)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/William/IdeaProjects/aurora4spark/aurora4spark-parent/aurora4spa..., PartitionFilters: [], PushedFilters: [IsNotNull(userId)], ReadSchema: struct<userId:string,totalPrice:string>
/* 018 */  */
/* 019 */ // codegenStageId=4
/* 020 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 021 */   private Object[] references;
/* 022 */   private scala.collection.Iterator[] inputs;
/* 023 */   private boolean sort_needToSort_0;
/* 024 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;
/* 025 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;
/* 026 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;
/* 027 */   private scala.collection.Iterator inputadapter_input_0;
/* 028 */
/* 029 */   public GeneratedIteratorForCodegenStage4(Object[] references) {
/* 030 */     this.references = references;
/* 031 */   }
/* 032 */
/* 033 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 034 */     partitionIndex = index;
/* 035 */     this.inputs = inputs;
/* 036 */     sort_needToSort_0 = true;
/* 037 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();
/* 038 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();
/* 039 */
/* 040 */     inputadapter_input_0 = inputs[0];
/* 041 */
/* 042 */   }
/* 043 */
/* 044 */   private void sort_addToSorter_0() throws java.io.IOException {
/* 045 */     // PRODUCE: InputAdapter
/* 046 */     while ( inputadapter_input_0.hasNext()) {
/* 047 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 048 */
/* 049 */       // CONSUME: Sort [joined_timestamp#3L ASC NULLS FIRST], true, 0
/* 050 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);
/* 051 */       // shouldStop check is eliminated
/* 052 */     }
/* 053 */
/* 054 */   }
/* 055 */
/* 056 */   protected void processNext() throws java.io.IOException {
/* 057 */     // PRODUCE: Sort [joined_timestamp#3L ASC NULLS FIRST], true, 0
/* 058 */     if (sort_needToSort_0) {
/* 059 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();
/* 060 */       sort_addToSorter_0();
/* 061 */       sort_sortedIter_0 = sort_sorter_0.sort();
/* 062 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);
/* 063 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());
/* 064 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);
/* 065 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());
/* 066 */       sort_needToSort_0 = false;
/* 067 */     }
/* 068 */
/* 069 */     while ( sort_sortedIter_0.hasNext()) {
/* 070 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();
/* 071 */
/* 072 */       // CONSUME: WholeStageCodegen (4)
/* 073 */       append(sort_outputRow_0);
/* 074 */
/* 075 */       if (shouldStop()) return;
/* 076 */     }
/* 077 */   }
/* 078 */
/* 079 */ }



+-------------------------------+------+----------------+
|sum(CAST(totalPrice AS DOUBLE))|userId|joined_timestamp|
+-------------------------------+------+----------------+
|                         1205.0|    11|      1581651894|
|                          819.0|    18|      1591231894|
|                          463.0|    14|      1591321894|
|                         1208.0|     1|      1591451894|
|                          133.0|     4|      1591551894|
|                          858.0|     6|      1591612345|
|                         1699.0|     3|      1591621894|
|                          393.0|    19|      1591642394|
|                          186.0|    13|      1591642894|
|                          901.0|    17|      1591699894|
+-------------------------------+------+----------------+
only showing top 10 rows

