# Caching of Compiled Native Code by Semantic Identity


## Status

PROPOSED AND IMPLEMENTED

Proposed by: Benson Ma (06/20/2022)

Discussed with: Eduardo Gonzalez, Paul Dubs


## Context

When Spark SQL queries are executed, the Spark Cyclone plugin first intercepts
the SQL plan tree generated by Spark, and replaces certain plan nodes with
substitutes that contain generated native C code for the operation that the
node represents (e.g. SQL `filter`).  A `ColumnarRule` is then applied to the
plan tree to compile all the C code attached and transform the tree such that
plan nodes containing C code will contain library references instead.  Finally,
When the plan node is executed, the library references are loaded by AVEO for
execution.

Because native code generation is tied to the definition and identity of the
plan, the generated C code is unique.  Even if two plans are functionally
equivalent, (e.g. "merge columnar batches of (`Long`, `Float`, `String`) rows"),
the plans are assigned id's from Spark, and so the name of the generated C
function will reflect this.  As code compilation is a slow process (on the order
of 2-4 seconds per `.SO` generation), it is desirable to be able to cache
compiled code, but the uniqueness of the generated code prevents effective
lookup of the cache for functionally equivalent code that was previously
generated and compiled.  This is especially problematic for efficient Spark SQL
streaming on the VE, where the same query is re-compiled and run repeatedly
despite the query remaining the same across each incoming batch.


## Proposal

Rather than caching compiled code with a hash that is based on the string value
of the generated code that was compiled, we cache the code based on a hash of
the "semantic identity" of the function being compiled.  For example, if we know
the function to be effectively "merge columnar batches of (`Long`, `Float`,
`String`) rows", we can derive a hash ID of this as:

```scala
("merge", VeNullableLong, VeNullableInt, VeString).hashCode
```

Because much of the existing infrastructure around native code generation is
defined as code generation templates that take in certain value type parameters
and generate C code based on those parameters, we can take advantage of this and
easily retrofit semantic identity-based function hashing into these classes.

### Details

The proposal and changes are described in greater detail below.

#### `CFunction`

A `CFunction` is an 1:1 mapped abstraction of a C function definition.  It
contains everything needed to be able to render into a compilable C source:

```scala
trait CFunction {
  def headers: Set[CFunction.IncludeHeader]

  def declaration: CodeLines

  def definition: CodeLines
}
```

**NOTE** that this definition is different from the current implementation of
`CFunction` and `CFunction2`, and future work will converge the existing code to
approximate this definition.

#### `NativeFunction`

A `NativeFunction` is the basic block of cache-able compiled code.  It contains
one primary `CFunction` and potentially multiple secondary `Cfunction`s that the
primary `CFunction` will call to.  The secondary functions concept is added to
support existing code generation for certain SQL plans, like those involving
joinss.

```scala
trait NativeFunction {
  final def identifier: String = primary.name

  def hashId: Int

  def primary: CFunction2

  def secondary: Seq[CFunction2]
}
```

The `identifier` is the name of the primary `CFunction`, and is to be the only
symbol AVEO will be calling from the `.So` that contains the compiled
`NativeFunction`.  Lastly, a `NativeFunction` contains a `hashId`, which should
be implemented as a hash value of the semantic identity of the function.

#### `NativeCodeCompiler`

A `NativeCodeCompiler` takes a list of `NativeFunction`s, builds the code
embedded in the `NativeFunction`s, and returns a map, where the keys are the
`hashId`s of the input `NativeFunction`s and the values are the compiled code
information for the associated `NativeFunction`:

```scala
case class CompiledCodeInfo(hashId: Int,
                            name: String,
                            path: Path)

trait NativeCodeCompiler {
  def cwd: Path

  def build(functions: Seq[NativeFunction]): Map[Int, CompiledCodeInfo]
}
```

Most relevant is the `CachingNativeCodeCompiler`, which intercepts build
requests to the underlying `NativeCodeCompiler` and short-circuits to return
cached `CompiledCodeInfo` for the subset of `NativeFunction`s whose `hashId`s it
has already seen before:

```scala
final case class CachingNativeCodeCompiler(underlying: NativeCodeCompiler,
                                           buildcache: MMap[Int, CompiledCodeInfo] = MMap.empty)
                                           extends NativeCodeCompiler
```

#### Compilation and Indexing

Upon invocation of `CachingNativeCodeCompiler::build()`, the subset of
`NativeFunction`s whose `hashId`s have not been cached will be compiled into a
single `.SO` in the plugin's VE kernel build directory.  In addition, a
corresponding comopilation index (with the same file name but with extension
`*.so.cidx`) will be written to the directory.  The compilation index is a plain
text file which contains the `hashId` and `identifier` of the  `NativeFunction`s
that were compiled into the `.SO`:

```sh
[bm@localhost ~]$ ls /opt/spark/work/cyclone/_spark_1039551203.so
/opt/spark/work/cyclone/_spark_1039551203.so

[bm@localhost ~]$ cat /opt/spark/work/cyclone/_spark_1039551203.so.cidx
1545868686,final_eval_734128758
-1999057355,final_eval_947146003
-1383308426,merge_eval_947146003
-2017203877,partial_eval_734128758
143981954,partial_eval_947146003
```

When the Spark Cyclone plugin is launched, the `CachingNativeCodeCompiler` will
scan the assigned VE kernel build directory for `*.so.cidx` files and build its
initial cache from them.  Thus, it is possible to keep the benefits of cached
compilation across Spark session restarts by specifying the same
`spark.com.nec.spark.kernel.directory` configuration.

#### `VeFunction` and `VeFunctionTemplate`

A `VeFunction` is an abstraction over native code (both compiled and uncompiled)
that is contained in a VE-based `SparkPlan`, and are also used to facilitate
calls to the VE during plan execution:

```scala
sealed trait VeFunctionStatus
final case class RawCode(code: String) extends VeFunctionStatus
final case class SourceCode(function: NativeFunction) extends VeFunctionStatus
final case class Compiled(location: LibLocation) extends VeFunctionStatus

final case class VeFunction(status: VeFunctionStatus,
                            name: String,
                            outputs: Seq[CVector])
```

`VeFunction`s are generated by `VeFunctionTemplate`s, which most of the existing
native code generation infrastructure is implemented as.  The existing
`VeFunction` is extended to support `SourceCode(NativeFunction)` as one of the
`VeFunctionStatus`s.

As an example, the merge function template is defined as follows (abridged):

```scala
final case class MergeFunction(name: String,
                               columns: Seq[VeType]) extends VeFunctionTemplate {
  def hashId: Int = {
    (getClass.getName, columns).hashCode
  }
}
```

The `hashId` is defined to ensure that it is dependent on only the types of the
columns being merged and has no dependencies on the name of the function, which
is dependent on the plan ID.

#### `CombinedCompilationColumnarRule`

Likewise, the existing `ColumnarRule` for transforming uncompiled to compiled
`VeFunction`s is extended to support compilation of `NativeFunction`-containing
`VeFunction`s using the `CachingNativeCodeCompiler`.  The
`Map[Int, CompiledCodeInfo]` returned from `CachingNativeCodeCompiler::build()`
is then used to search and replace `VeFunction`s embedded in the plans with
compiled variants.


## Alternatives

### Change the Way Plan IDs are Assigned

Since the string uniquess of the generated C code comes mainly from the
uniqueness of the `SparkPlan` plan IDs, one alternative is to change the method
through which plan ID's are generated during Spark SQL execution.  An attempt
was previously made in this direction, and proved to be difficult if not
impossible, because the plan ID's are dependent on the plan bodies, which are
dependent to the text of the SQL query, and are recursive.

### Keep the Status Quo

Another alternative is to not cache built native code at all.  This of course
comes at the cost of not being able to use the VE efficiently to perform Spark
SQL queries.


## Consequences

### Advantages

* We can speed up SQL queries on the VE over time, if functionally same query
  plans are used over and over again.
* We can significantly speed up Spark SQL streaming on the VE, by compiling
  native code exactly once and using the cached code afterwards.

### Disadvantages

* Understanding Spark Cyclone's new native code compilation architecture may
  require more effort than previously.
* Caching compiled code across Spark session restarts may cause confusing and
  illegal behavior if the code generation infrastructure and/or `libcyclone.so`
  changes between sessions.  This can be avoided by clearing the VE kernel build
  directory.
